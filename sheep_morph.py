import streamlit as st
import base64
import zipfile
import os
import shutil
from tensorflow.keras.models import Sequential, load_model, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D
from tensorflow.keras.regularizers import l2
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from tensorflow.keras.applications import VGG19
from tensorflow.keras.applications.vgg19 import preprocess_input
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import tensorflow as tf
from PIL import Image
import io
import time
from datetime import datetime

# ğŸŒ Variables globales - Comme des "boÃ®tes de rangement" partagÃ©es dans toute l'application
model = None  # Notre modÃ¨le de classification - comme un cerveau entraÃ®nÃ©
model_path = "saved_model.h5"  # Adresse oÃ¹ sauvegarder notre cerveau
class_names = []  # Liste des noms de classes - comme un dictionnaire

# ğŸ¨ Configuration de la page Streamlit - DÃ©coration de notre interface
st.set_page_config(page_title="SHEEP MORPH - photo style morphing", layout="wide")

# ğŸ·ï¸ Interface utilisateur - Titres et prÃ©sentation
st.markdown(
    """
    <h1 style="text-align: center; color: black;">SAFFIRE morphing</h1>
    """,
    unsafe_allow_html=True
)

st.markdown(
    """
    <h2 style="text-align: center; color: black;">Powered by AI and TensorFlow</h2>
    """,
    unsafe_allow_html=True
)

st.markdown(
    """
    <p style="text-align: center; color: black;">Changez le style de vos photos</p>
    """,
    unsafe_allow_html=True
)

# ğŸ–¼ï¸ Configuration de l'image de fond - Comme changer le papier peint
background_image_path = "background.jpg"

if os.path.exists(background_image_path):
    with open(background_image_path, "rb") as image_file:
        # Conversion de l'image en format web (Base64) - comme traduire une langue
        encoded_string = base64.b64encode(image_file.read()).decode()
    
    # Injection du CSS pour l'arriÃ¨re-plan - comme peindre les murs
    st.markdown(
        f"""
        <style>
        .stApp {{
            background-image: url(data:image/png;base64,{encoded_string});
            background-size: cover;
            background-repeat: no-repeat;
            background-attachment: fixed;
        }}
        </style>
        """,
        unsafe_allow_html=True
    )

# ğŸ›ï¸ Interface de contrÃ´le principale - Menu de navigation
st.sidebar.header("Configuration")
main_mode = st.sidebar.radio("SÃ©lectionnez le module:", ["Classification", "Transfert de Style"])

# ğŸ“Š MODULE DE CLASSIFICATION
if main_mode == "Classification":
    mode = st.sidebar.radio("Select Mode:", ["Automatic", "Manual"])
    
    # Logo dans la barre latÃ©rale - DÃ©coration
    if os.path.exists("logo.jpg"):
        st.sidebar.image("logo.jpg", width=150, caption="SAFFIRE")

    # ğŸ“¦ Section de chargement des donnÃ©es
    st.markdown("## Chargement des DonnÃ©es")
    train_data = st.file_uploader("Importer les donnÃ©es d'entraÃ®nement (ZIP)", type=["zip"])
    train_dir = "temp_train_dir"

    def extract_zip(zip_file, extract_to):
        """
        ğŸ—‚ï¸ Fonction d'extraction intelligente du ZIP
        
        Analogie : Comme dÃ©baller un colis avec plusieurs boÃ®tes imbriquÃ©es.
        Cette fonction est assez intelligente pour comprendre si votre ZIP a
        une boÃ®te supplÃ©mentaire Ã  l'intÃ©rieur et l'enlÃ¨ve automatiquement.
        
        Args:
            zip_file: Le fichier ZIP Ã  dÃ©baller
            extract_to: OÃ¹ mettre le contenu dÃ©ballÃ©
        """
        # ğŸ§¹ Nettoyage prÃ©alable - Comme vider une boÃ®te avant de la remplir
        if os.path.exists(extract_to):
            shutil.rmtree(extract_to)
        os.makedirs(extract_to)
        
        with zipfile.ZipFile(zip_file, 'r') as zip_ref:
            # ğŸ“‚ Extraction dans un dossier temporaire - Zone de tri
            temp_extract = extract_to + "_temp"
            zip_ref.extractall(temp_extract)
            
            # ğŸ” Analyse de la structure - Comme inspecter le contenu d'un colis
            items = os.listdir(temp_extract)
            
            # Cas 1: Structure directe (parfaite) - Comme un colis bien organisÃ©
            if all(os.path.isdir(os.path.join(temp_extract, item)) for item in items if not item.startswith('.')):
                for item in items:
                    if not item.startswith('.') and not item == '__MACOSX':
                        source_path = os.path.join(temp_extract, item)
                        dest_path = os.path.join(extract_to, item)
                        if os.path.isdir(source_path):
                            shutil.copytree(source_path, dest_path)
                        else:
                            shutil.copy2(source_path, dest_path)
            
            # Cas 2: Dossier parent en trop - Comme une boÃ®te dans une boÃ®te
            else:
                parent_folder = None
                for item in items:
                    item_path = os.path.join(temp_extract, item)
                    if os.path.isdir(item_path) and not item.startswith('.') and item != '__MACOSX':
                        sub_items = os.listdir(item_path)
                        if any(os.path.isdir(os.path.join(item_path, sub)) for sub in sub_items if not sub.startswith('.')):
                            parent_folder = item_path
                            break
                
                if parent_folder:
                    # Extraction du contenu de la boÃ®te interne
                    for item in os.listdir(parent_folder):
                        if not item.startswith('.') and not item == '__MACOSX':
                            source_path = os.path.join(parent_folder, item)
                            dest_path = os.path.join(extract_to, item)
                            if os.path.isdir(source_path):
                                shutil.copytree(source_path, dest_path)
                            else:
                                shutil.copy2(source_path, dest_path)
                else:
                    # Structure non reconnue - Copie tout tel quel
                    for item in items:
                        if not item.startswith('.') and not item == '__MACOSX':
                            source_path = os.path.join(temp_extract, item)
                            dest_path = os.path.join(extract_to, item)
                            if os.path.isdir(source_path):
                                shutil.copytree(source_path, dest_path)
                            else:
                                shutil.copy2(source_path, dest_path)
            
            # ğŸ§¹ Nettoyage du dossier temporaire - Ranger aprÃ¨s le tri
            shutil.rmtree(temp_extract)
            
            # ğŸ“Š Rapport de ce qui a Ã©tÃ© trouvÃ© - Inventaire
            classes_found = [d for d in os.listdir(extract_to) if os.path.isdir(os.path.join(extract_to, d))]
            st.info(f"ğŸ“ Structure dÃ©tectÃ©e : {len(classes_found)} classes trouvÃ©es")
            st.write("Classes dÃ©tectÃ©es :", ", ".join(classes_found))

    # âš™ï¸ Configuration des hyperparamÃ¨tres selon le mode
    if mode == "Manual":
        st.sidebar.markdown("### ğŸ›ï¸ HyperparamÃ¨tres d'EntraÃ®nement")
        
        # ğŸ§  Optimiseur - Le "professeur" qui guide l'apprentissage
        optimizer_choice = st.sidebar.selectbox(
            "Optimiseur (le 'professeur' de votre IA):", 
            ("Adam", "SGD", "RMSprop"),
            help="Adam = professeur patient et intelligent | SGD = professeur simple mais efficace | RMSprop = professeur qui s'adapte"
        )
        
        # ğŸ“ Taux d'apprentissage - La "vitesse d'apprentissage"
        learning_rate = st.sidebar.number_input(
            "Taux d'apprentissage (vitesse d'apprentissage):", 
            min_value=0.0001, max_value=1.0, value=0.001, step=0.0001, format="%f",
            help="Trop rapide = votre IA devient confuse | Trop lent = votre IA apprend trÃ¨s lentement"
        )
        
        # ğŸ”„ Epochs - Nombre de "cours" complets
        epochs = st.sidebar.number_input(
            "Nombre d'epochs (cours complets):", 
            min_value=1, max_value=100, value=30, step=1,
            help="Comme le nombre de fois que votre IA revoit tout le manuel d'apprentissage"
        )
        
        # ğŸ“¦ Taille des batchs - Taille des "groupes d'Ã©tude"
        batch_size = st.sidebar.number_input(
            "Taille des batchs (taille des groupes d'Ã©tude):", 
            min_value=1, max_value=128, value=32, step=1,
            help="Nombre d'images que votre IA Ã©tudie en mÃªme temps. Plus grand = plus rapide mais plus de mÃ©moire"
        )
        
        # ğŸ—ï¸ Architecture du rÃ©seau neuronal
        st.sidebar.markdown("### ğŸ—ï¸ Architecture du RÃ©seau")
        
        # Nombre de couches convolutives - "Ã‰tages de dÃ©tection"
        num_conv_layers = st.sidebar.slider(
            "Nombre de couches convolutives (Ã©tages de dÃ©tection):", 
            min_value=1, max_value=5, value=3,
            help="Chaque Ã©tage dÃ©tecte des patterns plus complexes : 1=lignes, 2=formes, 3=objets"
        )
        
        # Filtres par couche - "Nombre de dÃ©tecteurs par Ã©tage"
        filters_per_layer = []
        for i in range(num_conv_layers):
            filters = st.sidebar.number_input(
                f"Filtres couche {i+1} (dÃ©tecteurs Ã  l'Ã©tage {i+1}):", 
                min_value=8, max_value=512, value=16 * (2**i), step=8,
                help=f"Ã‰tage {i+1}: Plus de dÃ©tecteurs = plus de prÃ©cision mais plus lent"
            )
            filters_per_layer.append(filters)
        
        # Neurones dans la couche dense - "Taille du cerveau de dÃ©cision"
        dense_units = st.sidebar.number_input(
            "Neurones Dense (taille du cerveau de dÃ©cision):", 
            min_value=8, max_value=512, value=64, step=8,
            help="Le cerveau final qui prend la dÃ©cision. Plus gros = plus intelligent mais plus lent"
        )
        
        # Taux de Dropout - "Pourcentage d'oubli volontaire"
        dropout_rate = st.sidebar.slider(
            "Taux de Dropout (oubli volontaire pour Ã©viter le par-cÅ“ur):", 
            min_value=0.0, max_value=0.9, value=0.5, step=0.05,
            help="Comme dire Ã  votre IA d'oublier 50% de ce qu'elle voit pour ne pas apprendre par cÅ“ur"
        )
        
        # Fonction d'activation - "Type de rÃ©flexion"
        activation_function = st.sidebar.selectbox(
            "Fonction d'activation (type de rÃ©flexion):", 
            ("relu", "sigmoid", "tanh"),
            help="ReLU = pensÃ©e simple et rapide | Sigmoid = pensÃ©e nuancÃ©e | Tanh = pensÃ©e Ã©quilibrÃ©e"
        )
        
        # ğŸ›¡ï¸ RÃ©gularisation avancÃ©e
        st.sidebar.markdown("### ğŸ›¡ï¸ RÃ©gularisation AvancÃ©e")
        
        # RÃ©gularisation L2 - "PÃ©nalitÃ© pour la complexitÃ©"
        l2_regularization = st.sidebar.slider(
            "RÃ©gularisation L2 (pÃ©nalitÃ© complexitÃ©):", 
            min_value=0.0, max_value=0.01, value=0.0001, step=0.0001, format="%.4f",
            help="EmpÃªche votre IA de devenir trop compliquÃ©e. Comme limiter le nombre de rÃ¨gles qu'elle peut apprendre"
        )
        
        # Patience pour l'arrÃªt prÃ©coce
        early_stopping_patience = st.sidebar.number_input(
            "Patience arrÃªt prÃ©coce (patience avant abandon):", 
            min_value=5, max_value=50, value=10, step=1,
            help="Nombre d'epochs sans amÃ©lioration avant d'arrÃªter l'entraÃ®nement automatiquement"
        )
        
        # RÃ©duction du learning rate
        reduce_lr_patience = st.sidebar.number_input(
            "Patience rÃ©duction LR (patience avant ralentissement):", 
            min_value=3, max_value=20, value=5, step=1,
            help="Si pas d'amÃ©lioration, ralentir l'apprentissage au lieu d'abandonner"
        )
        
        reduce_lr_factor = st.sidebar.slider(
            "Facteur rÃ©duction LR (niveau de ralentissement):", 
            min_value=0.1, max_value=0.9, value=0.5, step=0.1,
            help="De combien ralentir l'apprentissage (0.5 = moitiÃ© moins vite)"
        )
        
    else:
        # ğŸ¤– Mode automatique - Configuration optimisÃ©e par dÃ©faut
        st.sidebar.markdown("### ğŸ¤– Configuration Automatique OptimisÃ©e")
        st.sidebar.write("âœ… ParamÃ¨tres optimaux sÃ©lectionnÃ©s automatiquement")
        
        # Valeurs par dÃ©faut optimisÃ©es
        optimizer_choice = "Adam"
        learning_rate = 0.001
        epochs = 20
        batch_size = 32
        num_conv_layers = 3
        filters_per_layer = [32, 64, 128]
        dense_units = 128
        dropout_rate = 0.3
        activation_function = "relu"
        l2_regularization = 0.0001
        early_stopping_patience = 10
        reduce_lr_patience = 5
        reduce_lr_factor = 0.5

    # ğŸš€ Bouton d'entraÃ®nement
    if st.button("ğŸš€ DÃ©marrer l'entraÃ®nement", type="primary"):
        if train_data is not None:
            # ğŸ“¦ Extraction et prÃ©paration des donnÃ©es
            extract_zip(train_data, train_dir)
            
            # ğŸ­ Augmentation des donnÃ©es - "CrÃ©er des variations"
            datagen = ImageDataGenerator(
                rescale=1./255,  # Normalisation - ramener les valeurs entre 0 et 1
                validation_split=0.2,  # 20% pour tester, 80% pour apprendre
                # Options d'augmentation possibles (commentÃ©es pour le mode de base)
                # rotation_range=20,  # Rotation alÃ©atoire
                # width_shift_range=0.2,  # DÃ©calage horizontal
                # height_shift_range=0.2,  # DÃ©calage vertical
                # horizontal_flip=True  # Miroir horizontal
            )
            
            # ğŸ­ CrÃ©ation des gÃ©nÃ©rateurs de donnÃ©es - "ChaÃ®nes de production"
            train_generator = datagen.flow_from_directory(
                train_dir, 
                target_size=(128, 128),  # Redimensionner toutes les images
                batch_size=batch_size, 
                class_mode='categorical',  # Classification multi-classes
                subset='training'
            )
            
            val_generator = datagen.flow_from_directory(
                train_dir, 
                target_size=(128, 128), 
                batch_size=batch_size, 
                class_mode='categorical', 
                subset='validation'
            )
            
            # ğŸ“Š Analyse des donnÃ©es
            num_classes = len(train_generator.class_indices)
            class_names = list(train_generator.class_indices.keys())
            st.info(f"ğŸ¯ {num_classes} classes dÃ©tectÃ©es : {', '.join(class_names)}")

            # ğŸ§  SÃ©lection de l'optimiseur - "Choix du professeur"
            if optimizer_choice == "Adam":
                # Adam = Professeur intelligent qui s'adapte
                optimizer = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999)
            elif optimizer_choice == "SGD":
                # SGD = Professeur traditionnel mais efficace
                optimizer = SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)
            elif optimizer_choice == "RMSprop":
                # RMSprop = Professeur qui s'adapte aux difficultÃ©s
                optimizer = RMSprop(learning_rate=learning_rate, rho=0.9)

            # ğŸ—ï¸ Construction du modÃ¨le - "Assemblage du cerveau"
            model = Sequential()
            
            # ğŸ‘ï¸ Couches convolutives - "Ã‰tages de dÃ©tection visuelle"
            for i, filters in enumerate(filters_per_layer):
                model.add(Conv2D(
                    filters, (3, 3),  # Filtres 3x3 comme des petites loupes
                    activation=activation_function,
                    kernel_regularizer=l2(l2_regularization),  # Ã‰viter la sur-spÃ©cialisation
                    padding='same',  # Garder la mÃªme taille d'image
                    name=f'conv2d_{i+1}'
                ))
                
                # ğŸ§¼ Normalisation par batch - "Standardisation"
                model.add(BatchNormalization(name=f'batch_norm_{i+1}'))
                
                # ğŸ”½ Max Pooling - "RÃ©sumÃ© de zone"
                model.add(MaxPooling2D(pool_size=(2, 2), name=f'max_pool_{i+1}'))
            
            # ğŸŒ Globalisation des informations
            model.add(GlobalAveragePooling2D(name='global_avg_pool'))
            
            # ğŸ§  Couche de dÃ©cision dense
            model.add(Dense(
                dense_units, 
                activation=activation_function,
                kernel_regularizer=l2(l2_regularization),
                name='dense_decision'
            ))
            
            # ğŸ² Dropout - "Oubli volontaire pour Ã©viter le par-cÅ“ur"
            model.add(Dropout(dropout_rate, name='dropout'))
            
            # ğŸ¯ Couche de sortie finale
            model.add(Dense(
                num_classes, 
                activation='softmax',  # ProbabilitÃ©s qui totalisent 100%
                name='output'
            ))

            # âš™ï¸ Compilation du modÃ¨le - "Configuration finale"
            model.compile(
                optimizer=optimizer,
                loss='categorical_crossentropy',  # Fonction de coÃ»t pour classification
                metrics=['accuracy']  # Mesurer la prÃ©cision
            )
            
            # ğŸ“Š Affichage de l'architecture
            st.write("ğŸ—ï¸ **Architecture du modÃ¨le crÃ©Ã© :**")
            model_summary = []
            model.summary(print_fn=lambda x: model_summary.append(x))
            st.text('\n'.join(model_summary))

            # ğŸ“ EntraÃ®nement avec callbacks - "Supervision intelligente"
            from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
            
            callbacks = [
                # ğŸ›‘ ArrÃªt prÃ©coce si pas d'amÃ©lioration
                EarlyStopping(
                    monitor='val_accuracy',
                    patience=early_stopping_patience,
                    restore_best_weights=True,
                    verbose=1
                ),
                # ğŸ“‰ RÃ©duction du learning rate si plateau
                ReduceLROnPlateau(
                    monitor='val_accuracy',
                    factor=reduce_lr_factor,
                    patience=reduce_lr_patience,
                    min_lr=1e-7,
                    verbose=1
                )
            ]
            
            # ğŸš€ Lancement de l'entraÃ®nement
            with st.spinner('ğŸ“ EntraÃ®nement en cours... Votre IA apprend !'):
                history = model.fit(
                    train_generator,
                    epochs=epochs,
                    validation_data=val_generator,
                    callbacks=callbacks,
                    verbose=1
                )
            
            # ğŸ’¾ Sauvegarde du modÃ¨le entraÃ®nÃ©
            model.save(model_path)
            st.success("âœ… EntraÃ®nement terminÃ© et modÃ¨le sauvegardÃ© !")

            # ğŸ“ˆ Affichage des courbes de convergence - "Graphiques de progression"
            st.markdown("## ğŸ“ˆ Courbes de Convergence")
            fig, ax = plt.subplots(1, 2, figsize=(14, 5))
            
            # Graphique de la perte
            ax[0].plot(history.history['loss'], label='Perte EntraÃ®nement', color='blue')
            ax[0].plot(history.history['val_loss'], label='Perte Validation', color='red')
            ax[0].set_title('Ã‰volution de la Perte (plus bas = mieux)')
            ax[0].set_xlabel('Epochs')
            ax[0].set_ylabel('Perte')
            ax[0].legend()
            ax[0].grid(True, alpha=0.3)

            # Graphique de la prÃ©cision
            ax[1].plot(history.history['accuracy'], label='PrÃ©cision EntraÃ®nement', color='green')
            ax[1].plot(history.history['val_accuracy'], label='PrÃ©cision Validation', color='orange')
            ax[1].set_title('Ã‰volution de la PrÃ©cision (plus haut = mieux)')
            ax[1].set_xlabel('Epochs')
            ax[1].set_ylabel('PrÃ©cision')
            ax[1].legend()
            ax[1].grid(True, alpha=0.3)
            
            st.pyplot(fig)

            # ğŸ¯ Matrice de confusion - "Tableau des erreurs"
            st.markdown("## ğŸ¯ Matrice de Confusion")
            if val_generator.samples > 0:
                # PrÃ©dictions sur les donnÃ©es de validation
                val_preds = model.predict(val_generator, verbose=0)
                y_pred = np.argmax(val_preds, axis=1)
                y_true = val_generator.classes

                # Mise Ã  jour des noms de classes
                if not class_names:
                    class_names = list(val_generator.class_indices.keys())

                # CrÃ©ation et affichage de la matrice
                cm = confusion_matrix(y_true, y_pred)
                fig_cm, ax_cm = plt.subplots(figsize=(8, 6))
                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
                disp.plot(ax=ax_cm, cmap='Blues', colorbar=True)
                ax_cm.set_title("Matrice de Confusion\n(Diagonale = bonnes prÃ©dictions)")
                st.pyplot(fig_cm)
                
                # Statistiques dÃ©taillÃ©es
                accuracy = np.trace(cm) / np.sum(cm)
                st.write(f"**PrÃ©cision globale :** {accuracy:.2%}")
                
            else:
                st.warning("âš ï¸ Aucune donnÃ©e de validation disponible.")

    # ğŸ”® Section de prÃ©diction
    st.markdown("## ğŸ”® PrÃ©diction sur une Image")
    image_file = st.file_uploader("Choisissez une image pour prÃ©diction", type=["jpg", "png", "jpeg"])
    
    if image_file and st.button("ğŸ”® PrÃ©dire", type="secondary"):
        # Chargement du modÃ¨le si nÃ©cessaire
        if model is None and os.path.exists(model_path):
            model = load_model(model_path)
            
        if model is not None:
            # PrÃ©processing de l'image
            img = load_img(image_file, target_size=(128, 128))
            img_array = img_to_array(img)
            img_array = np.expand_dims(img_array, axis=0) / 255.0  # Normalisation
            
            # PrÃ©diction
            prediction = model.predict(img_array)[0]
            predicted_class = np.argmax(prediction)
            predicted_label = class_names[predicted_class] if class_names else f"Classe {predicted_class}"
            confidence = round(prediction[predicted_class] * 100, 2)
            
            # Affichage des rÃ©sultats
            col1, col2 = st.columns(2)
            with col1:
                st.image(image_file, caption="ğŸ–¼ï¸ Image Ã  analyser")
            with col2:
                st.markdown("### ğŸ¯ RÃ©sultat de la PrÃ©diction")
                st.success(f"**Classe prÃ©dite :** {predicted_label}")
                st.info(f"**Confiance :** {confidence}%")
                
                # Barre de progression pour la confiance
                st.progress(confidence / 100)
                
                # Top 3 des prÃ©dictions
                if len(prediction) > 1:
                    st.markdown("#### ğŸ† Top 3 des prÃ©dictions")
                    top_3_indices = np.argsort(prediction)[-3:][::-1]
                    for i, idx in enumerate(top_3_indices):
                        class_name = class_names[idx] if class_names else f"Classe {idx}"
                        score = prediction[idx] * 100
                        st.write(f"{i+1}. {class_name}: {score:.1f}%")
        else:
            st.error("â— Aucun modÃ¨le chargÃ©. Veuillez d'abord entraÃ®ner un modÃ¨le.")

# ğŸ¨ MODULE DE TRANSFERT DE STYLE
elif main_mode == "Transfert de Style":
    st.markdown("## ğŸ¨ Module de Transfert de Style Neural")
    st.write("*Transformez vos photos en Å“uvres d'art en combinant le contenu d'une image avec le style d'une autre*")
    
    # ğŸ›ï¸ HyperparamÃ¨tres principaux dans la sidebar
    st.sidebar.markdown("### ğŸ¨ ParamÃ¨tres Artistiques")
    
    # Poids du style - "IntensitÃ© artistique"
    style_weight = st.sidebar.slider(
        "Poids du style (intensitÃ© artistique):", 
        1e-2, 1e6, 1e4, step=1e3, format="%.0e",
        help="Plus Ã©levÃ© = plus artistique mais moins ressemblant Ã  l'original"
    )
    
    # Poids du contenu - "PrÃ©servation de l'original"
    content_weight = st.sidebar.slider(
        "Poids du contenu (prÃ©servation original):", 
        1e0, 1e4, 1e3, step=1e2, format="%.0e",
        help="Plus Ã©levÃ© = plus fidÃ¨le Ã  l'image originale"
    )
    
    # Nombre d'itÃ©rations - "Temps de crÃ©ation"
    iterations = st.sidebar.number_input(
        "Nombre d'itÃ©rations (temps de crÃ©ation):", 
        min_value=10, max_value=1000, value=100, step=10,
        help="Plus d'itÃ©rations = meilleure qualitÃ© mais plus lent"
    )
    
    # ğŸ”§ ParamÃ¨tres avancÃ©s
    st.sidebar.markdown("### ğŸ”§ ParamÃ¨tres AvancÃ©s")
    
    # Taux d'apprentissage - "Vitesse d'amÃ©lioration"
    learning_rate = st.sidebar.slider(
        "Taux d'apprentissage (vitesse amÃ©lioration):", 
        0.001, 0.1, 0.01, step=0.001,
        help="Plus rapide = convergence rapide mais risque d'instabilitÃ©"
    )
    
    # Taille maximale d'image - "QualitÃ© vs vitesse"
    max_image_size = st.sidebar.selectbox(
        "Taille max image (qualitÃ© vs vitesse):",
        [256, 384, 512, 768, 1024],
        index=2,  # 512 par dÃ©faut
        help="Plus grand = meilleure qualitÃ© mais beaucoup plus lent"
    )
    
    # ParamÃ¨tres de l'optimiseur Adam
    st.sidebar.markdown("#### âš™ï¸ Optimiseur Adam")
    
    beta1 = st.sidebar.slider(
        "Beta1 (mÃ©moire gradient):",
        0.8, 0.999, 0.99, step=0.01,
        help="MÃ©moire des gradients prÃ©cÃ©dents. Plus Ã©levÃ© = plus de mÃ©moire"
    )
    
    beta2 = st.sidebar.slider(
        "Beta2 (mÃ©moire variance):",
        0.9, 0.999, 0.999, step=0.001,
        help="MÃ©moire de la variance. GÃ©nÃ©ralement proche de 1.0"
    )
    
    epsilon = st.sidebar.selectbox(
        "Epsilon (stabilitÃ© numÃ©rique):",
        [1e-8, 1e-4, 1e-1, 1e-0],
        index=2,  # 1e-1 par dÃ©faut
        format_func=lambda x: f"{x:.0e}",
        help="Ã‰vite la division par zÃ©ro. Plus Ã©levÃ© = plus stable"
    )
    
    # ğŸšï¸ ContrÃ´le des couches de style
    st.sidebar.markdown("#### ğŸ¨ Couches de Style Actives")
    st.sidebar.write("*Chaque couche capture diffÃ©rents aspects du style*")
    
    use_block1 = st.sidebar.checkbox(
        "Block1 (textures fines)", 
        value=True,
        help="Capture les dÃ©tails fins : lignes, points, textures de base"
    )
    use_block2 = st.sidebar.checkbox(
        "Block2 (motifs simples)", 
        value=True,
        help="Capture les motifs simples : rayures, cercles, formes gÃ©omÃ©triques"
    )
    use_block3 = st.sidebar.checkbox(
        "Block3 (structures moyennes)", 
        value=True,
        help="Capture les structures moyennes : objets partiels, compositions"
    )
    use_block4 = st.sidebar.checkbox(
        "Block4 (formes complexes)", 
        value=True,
        help="Capture les formes complexes : objets entiers, relations spatiales"
    )
    use_block5 = st.sidebar.checkbox(
        "Block5 (composition globale)", 
        value=True,
        help="Capture la composition globale : distribution des Ã©lÃ©ments, style gÃ©nÃ©ral"
    )
    
    # ğŸ­ Options de prÃ©processing
    st.sidebar.markdown("#### ğŸ­ PrÃ©processing des Images")
    
    preserve_colors = st.sidebar.checkbox(
        "PrÃ©server couleurs originales",
        value=False,
        help="Garde les couleurs de l'image de contenu et applique seulement la texture du style"
    )
    
    enhance_contrast = st.sidebar.slider(
        "AmÃ©lioration contraste:",
        0.5, 2.0, 1.0, step=0.1,
        help="Ajuste le contraste de l'image finale. 1.0 = normal"
    )
    
    color_saturation = st.sidebar.slider(
        "Saturation couleurs:",
        0.0, 2.0, 1.0, step=0.1,
        help="Ajuste la vivacitÃ© des couleurs. 1.0 = normal"
    )
    
    # ğŸ” Options de dÃ©bogage
    st.sidebar.markdown("### ğŸ” DÃ©bogage et Analyse")
    
    debug_mode = st.sidebar.checkbox(
        "Mode dÃ©bogage", 
        value=True,
        help="Affiche les images prÃ©processÃ©es et aperÃ§us pendant l'entraÃ®nement"
    )
    
    show_diagnostics = st.sidebar.checkbox(
        "Diagnostic dÃ©taillÃ©", 
        value=False,
        help="Analyse technique des images uploadÃ©es (format, taille, valeurs)"
    )
    
    show_loss_breakdown = st.sidebar.checkbox(
        "DÃ©tail des pertes",
        value=False, 
        help="Affiche sÃ©parÃ©ment la perte de style et de contenu"
    )
    
    preview_frequency = st.sidebar.number_input(
        "FrÃ©quence aperÃ§us:",
        min_value=5, max_value=50, value=20, step=5,
        help="Montrer un aperÃ§u toutes les X itÃ©rations"
    )

    def diagnose_image(image_file, name):
        """
        ğŸ”¬ Fonction de diagnostic d'image
        
        Analogie : Comme un mÃ©decin qui examine un patient.
        Cette fonction regarde tous les "signes vitaux" de votre image :
        - Sa taille, son format, ses couleurs
        - DÃ©tecte s'il y a des problÃ¨mes potentiels
        
        Args:
            image_file: L'image Ã  diagnostiquer
            name: Nom de l'image pour l'affichage
        """
        img = Image.open(image_file)
        img_array = np.array(img)
        
        st.write(f"**ğŸ“Š Diagnostic de l'image {name}:**")
        st.write(f"- Format original: {img.format} {'âœ…' if img.format in ['JPEG', 'PNG'] else 'âš ï¸'}")
        st.write(f"- Mode couleur: {img.mode} {'âœ…' if img.mode == 'RGB' else 'âš ï¸'}")
        st.write(f"- Dimensions: {img.size[0]}Ã—{img.size[1]} pixels")
        st.write(f"- Forme du array: {img_array.shape}")
        st.write(f"- Type de donnÃ©es: {img_array.dtype}")
        st.write(f"- Plage valeurs: {img_array.min()} â†’ {img_array.max()}")
        
        # Analyse des couleurs par canal
        if len(img_array.shape) == 3:
            r_mean = img_array[:,:,0].mean()
            g_mean = img_array[:,:,1].mean()
            b_mean = img_array[:,:,2].mean()
            st.write(f"- Moyennes RGB: R={r_mean:.1f}, G={g_mean:.1f}, B={b_mean:.1f}")
            
            # DÃ©tection de problÃ¨mes potentiels
            if abs(r_mean - g_mean) < 5 and abs(g_mean - b_mean) < 5:
                st.warning("âš ï¸ Image semble en niveaux de gris")
            if img_array.max() <= 1:
                st.info("â„¹ï¸ Image dÃ©jÃ  normalisÃ©e [0,1]")
        
        return img_array

    # ğŸ¨ Fonctions pour le transfert de style
    @st.cache_resource
    def load_vgg_model():
        """
        ğŸ§  Chargement du modÃ¨le VGG19 prÃ©-entraÃ®nÃ©
        
        Analogie : Comme emprunter les yeux d'un expert en art.
        VGG19 a Ã©tÃ© entraÃ®nÃ© sur des millions d'images et "sait" reconnaÃ®tre
        les formes, textures et styles artistiques.
        
        Returns:
            ModÃ¨le VGG19 figÃ© (non-entraÃ®nable)
        """
        vgg = VGG19(include_top=False, weights='imagenet')
        vgg.trainable = False  # On fige le modÃ¨le - pas d'apprentissage
        return vgg

    def preprocess_image(image_path, max_dim=512):
        """
        ğŸ› ï¸ PrÃ©processing intelligent des images
        
        Analogie : Comme prÃ©parer une toile avant de peindre.
        Cette fonction nettoie, redimensionne et normalise l'image
        pour qu'elle soit parfaite pour le transfert de style.
        
        Args:
            image_path: Chemin vers l'image ou objet file Streamlit
            max_dim: Dimension maximale (plus grand = plus lent)
            
        Returns:
            Tensor TensorFlow normalisÃ© et redimensionnÃ©
        """
        if isinstance(image_path, str):
            # Chargement depuis fichier local
            img = tf.io.read_file(image_path)
            img = tf.image.decode_image(img, channels=3)
        else:
            # Chargement depuis Streamlit file uploader
            img = Image.open(image_path)
            img = np.array(img)
            
            # ğŸ¨ Gestion des diffÃ©rents formats d'image
            if len(img.shape) == 2:  # Image en niveaux de gris
                # Conversion en RGB en dupliquant le canal
                img = np.stack([img] * 3, axis=-1)
                st.info("â„¹ï¸ Image convertie de niveaux de gris vers RGB")
                
            elif len(img.shape) == 3 and img.shape[-1] == 4:  # RGBA (avec transparence)
                # Suppression du canal alpha (transparence)
                img = img[:, :, :3]
                st.info("â„¹ï¸ Canal alpha supprimÃ© (RGBA â†’ RGB)")
            
            # ğŸ“Š Normalisation intelligente des valeurs
            if img.dtype == np.uint8:
                # Conversion standard uint8 [0,255] â†’ float32 [0,1]
                img = img.astype(np.float32) / 255.0
            elif img.dtype == np.float32 and img.max() > 1.0:
                # Image float32 mal normalisÃ©e
                img = img / 255.0
                st.info("â„¹ï¸ Image float32 renormalisÃ©e")
            
            # Conversion en tensor TensorFlow
            img = tf.constant(img, dtype=tf.float32)
        
        # ğŸ”„ VÃ©rifications et conversions finales
        if img.dtype != tf.float32:
            img = tf.image.convert_image_dtype(img, tf.float32)
        
        # Assurer que l'image a exactement 3 canaux
        if len(img.shape) == 3 and img.shape[-1] != 3:
            if img.shape[-1] == 1:  # Niveaux de gris
                img = tf.image.grayscale_to_rgb(img)
            elif img.shape[-1] == 4:  # RGBA
                img = img[:, :, :3]
        
        # ğŸ“ Redimensionnement proportionnel
        # Analogie : Ajuster la taille d'une photo sans la dÃ©former
        shape = tf.cast(tf.shape(img)[:-1], tf.float32)
        long_dim = max(shape)
        scale = max_dim / long_dim
        
        new_shape = tf.cast(shape * scale, tf.int32)
        img = tf.image.resize(img, new_shape)
        
        # ğŸ“¦ Ajout de la dimension batch (pour le traitement par lots)
        img = img[tf.newaxis, :]
        
        # ğŸ›¡ï¸ SÃ©curitÃ© : s'assurer que les valeurs sont dans [0,1]
        img = tf.clip_by_value(img, 0.0, 1.0)
        
        return img

    def deprocess_image(processed_img):
        """
        ğŸ–¼ï¸ Conversion de l'image traitÃ©e vers format affichable
        
        Analogie : Comme dÃ©velopper une photo depuis un nÃ©gatif.
        Convertit le tensor TensorFlow normalisÃ© en image PNG/JPEG standard.
        
        Args:
            processed_img: Tensor TensorFlow [0,1]
            
        Returns:
            Array NumPy uint8 [0,255] prÃªt pour l'affichage
        """
        x = processed_img.copy()
        
        # Suppression de la dimension batch si prÃ©sente
        if len(x.shape) == 4:
            x = np.squeeze(x, 0)
        
        # ğŸ›¡ï¸ SÃ©curitÃ© : clipper les valeurs dans [0,1]
        x = np.clip(x, 0, 1)
        
        # ğŸ¨ Conversion vers format d'affichage [0,255]
        x = (x * 255).astype('uint8')
        
        return x

    def apply_color_adjustments(img, enhance_contrast, color_saturation):
        """
        ğŸ¨ Application d'ajustements colorimÃ©triques
        
        Analogie : Comme ajuster les rÃ©glages d'un tÃ©lÃ©viseur.
        Modifie le contraste et la saturation pour amÃ©liorer le rendu final.
        
        Args:
            img: Image Ã  ajuster
            enhance_contrast: Facteur de contraste (1.0 = normal)
            color_saturation: Facteur de saturation (1.0 = normal)
            
        Returns:
            Image ajustÃ©e
        """
        # Ajustement du contraste
        if enhance_contrast != 1.0:
            img = tf.image.adjust_contrast(img, enhance_contrast)
        
        # Ajustement de la saturation
        if color_saturation != 1.0:
            img = tf.image.adjust_saturation(img, color_saturation)
        
        # Re-clipper aprÃ¨s ajustements
        img = tf.clip_by_value(img, 0.0, 1.0)
        
        return img

    def gram_matrix(input_tensor):
        """
        ğŸ”¢ Calcul de la matrice de Gram pour capturer le style
        
        Analogie : Comme analyser les "empreintes digitales" artistiques.
        La matrice de Gram capture les corrÃ©lations entre diffÃ©rentes 
        caractÃ©ristiques visuelles, crÃ©ant une signature unique du style.
        
        Math : G[i,j] = Î£(F[k,i] Ã— F[k,j]) / N
        OÃ¹ F sont les features et N le nombre de positions
        
        Args:
            input_tensor: Features extraites par VGG19
            
        Returns:
            Matrice de Gram (corrÃ©lations de style)
        """
        # ğŸ“ VÃ©rification et ajustement des dimensions
        if len(input_tensor.shape) == 3:
            input_tensor = tf.expand_dims(input_tensor, 0)
        
        # ğŸ“Š Extraction des dimensions
        batch_size = tf.shape(input_tensor)[0]
        height = tf.shape(input_tensor)[1] 
        width = tf.shape(input_tensor)[2]
        channels = tf.shape(input_tensor)[3]
        
        # ğŸ”„ Reshape en matrice 2D : (positions, features)
        # Analogie : Comme Ã©taler toutes les "observations" en lignes
        features = tf.reshape(input_tensor, (batch_size, height * width, channels))
        
        # ğŸ§® Calcul de la matrice de Gram : F^T Ã— F
        # Analogie : Calculer toutes les corrÃ©lations entre features
        gram = tf.matmul(features, features, transpose_a=True)
        
        # â— Normalisation par le nombre de positions
        # Analogie : Faire une moyenne pour que la taille d'image n'influe pas
        num_locations = tf.cast(height * width, tf.float32)
        
        return gram / num_locations

    def build_style_layers_list():
        """
        ğŸ—ï¸ Construction de la liste des couches de style actives
        
        Retourne la liste des couches VGG19 Ã  utiliser selon les checkboxes
        """
        style_layers = []
        if use_block1:
            style_layers.append('block1_conv1')
        if use_block2:
            style_layers.append('block2_conv1') 
        if use_block3:
            style_layers.append('block3_conv1')
        if use_block4:
            style_layers.append('block4_conv1')
        if use_block5:
            style_layers.append('block5_conv1')
            
        return style_layers

    def style_content_loss(outputs, style_targets, content_targets, style_weight, content_weight):
        """
        âš–ï¸ Calcul de la perte combinÃ©e style + contenu
        
        Analogie : Comme noter un devoir avec deux critÃ¨res :
        - Respect du style artistique (originalitÃ©)
        - PrÃ©servation du contenu (fidÃ©litÃ©)
        
        Args:
            outputs: Sorties actuelles du modÃ¨le
            style_targets: Cibles de style (ce qu'on veut atteindre)
            content_targets: Cibles de contenu (ce qu'on veut prÃ©server)
            style_weight: Importance du style
            content_weight: Importance du contenu
            
        Returns:
            Perte totale Ã  minimiser
        """
        style_outputs = outputs['style']
        content_outputs = outputs['content']
        
        # ğŸ¨ Calcul de la perte de style
        # Analogie : Mesurer Ã  quel point le style diffÃ¨re de l'Å“uvre de rÃ©fÃ©rence
        style_loss = 0
        for name in style_targets.keys():
            # DiffÃ©rence quadratique entre matrices de Gram
            layer_loss = tf.reduce_mean((style_outputs[name] - style_targets[name])**2)
            style_loss += layer_loss
            
            # Debug : afficher les pertes par couche si demandÃ©
            if show_loss_breakdown:
                st.write(f"ğŸ¨ Perte style {name}: {float(layer_loss):.4f}")
        
        # Normalisation par le nombre de couches de style
        style_loss *= style_weight / len(style_targets)
        
        # ğŸ“· Calcul de la perte de contenu  
        # Analogie : Mesurer Ã  quel point on s'Ã©loigne de l'image originale
        content_loss = 0
        for name in content_targets.keys():
            # DiffÃ©rence quadratique entre features de contenu
            layer_loss = tf.reduce_mean((content_outputs[name] - content_targets[name])**2)
            content_loss += layer_loss
            
            if show_loss_breakdown:
                st.write(f"ğŸ“· Perte contenu {name}: {float(layer_loss):.4f}")
        
        # Normalisation par le nombre de couches de contenu
        content_loss *= content_weight / len(content_targets)
        
        # âš–ï¸ Perte totale = Style + Contenu
        total_loss = style_loss + content_loss
        
        # Debug dÃ©taillÃ©
        if show_loss_breakdown:
            st.write(f"**Total - Style: {float(style_loss):.4f}, Contenu: {float(content_loss):.4f}**")
        
        return total_loss

    class StyleContentModel(tf.keras.models.Model):
        """
        ğŸ­ ModÃ¨le d'extraction de style et contenu
        
        Analogie : Comme un critique d'art expert qui peut analyser
        sÃ©parÃ©ment le style artistique et le contenu d'une Å“uvre.
        
        Cette classe utilise VGG19 prÃ©-entraÃ®nÃ© pour extraire :
        - Les caractÃ©ristiques de style (matrices de Gram)
        - Les caractÃ©ristiques de contenu (features sÃ©mantiques)
        """
        
        def __init__(self, style_layers, content_layers):
            """
            ğŸ—ï¸ Initialisation du modÃ¨le extracteur
            
            Args:
                style_layers: Liste des couches VGG19 pour le style
                content_layers: Liste des couches VGG19 pour le contenu
            """
            super(StyleContentModel, self).__init__()
            
            # ğŸ§  Chargement du modÃ¨le VGG19 prÃ©-entraÃ®nÃ©
            self.vgg = load_vgg_model()
            self.style_layers = style_layers
            self.content_layers = content_layers
            self.num_style_layers = len(style_layers)
            self.vgg.trainable = False  # ModÃ¨le figÃ©
            
            # ğŸ”Œ Construction d'un extracteur unifiÃ© pour efficacitÃ©
            # Analogie : CrÃ©er un seul passage au lieu de plusieurs allers-retours
            style_outputs = [self.vgg.get_layer(name).output for name in style_layers]
            content_outputs = [self.vgg.get_layer(name).output for name in content_layers]
            model_outputs = style_outputs + content_outputs
            
            # ğŸ­ ModÃ¨le d'extraction unifiÃ©
            self.feature_extractor = tf.keras.Model([self.vgg.input], model_outputs)

        def call(self, inputs):
            """
            ğŸ”„ Forward pass - Extraction des caractÃ©ristiques
            
            Analogie : Passer une image dans les "yeux" de l'expert
            pour qu'il analyse le style et le contenu.
            
            Args:
                inputs: Image d'entrÃ©e normalisÃ©e [0,1]
                
            Returns:
                Dictionnaire avec features de style et contenu
            """
            # ğŸ“ VÃ©rification des dimensions d'entrÃ©e
            if len(inputs.shape) == 3:
                inputs = tf.expand_dims(inputs, 0)
            
            # ğŸ›¡ï¸ SÃ©curitÃ© : clipper les valeurs dans [0,1]
            inputs = tf.clip_by_value(inputs, 0.0, 1.0)
            
            # ğŸ¨ Ajustements colorimÃ©triques si demandÃ©s
            inputs = apply_color_adjustments(inputs, enhance_contrast, color_saturation)
            
            # ğŸ”„ PrÃ©processing pour VGG19
            # Conversion [0,1] â†’ [0,255] puis normalisation ImageNet
            inputs_scaled = inputs * 255.0
            preprocessed_input = preprocess_input(inputs_scaled)
            
            # ğŸ­ Extraction des features via le modÃ¨le unifiÃ©
            outputs = self.feature_extractor(preprocessed_input)
            
            # ğŸ“Š SÃ©paration des outputs style et contenu
            style_outputs = outputs[:self.num_style_layers]
            content_outputs = outputs[self.num_style_layers:]

            # ğŸ¨ Calcul des matrices de Gram pour le style
            style_features = []
            for i in range(self.num_style_layers):
                gram = gram_matrix(style_outputs[i])
                style_features.append(gram)

            # ğŸ“¦ Construction des dictionnaires de sortie
            content_dict = {}
            for i, content_name in enumerate(self.content_layers):
                content_dict[content_name] = content_outputs[i]

            style_dict = {}
            for i, style_name in enumerate(self.style_layers):
                style_dict[style_name] = style_features[i]

            return {'content': content_dict, 'style': style_dict}

    def perform_style_transfer(content_path, style_path, style_weight, content_weight, iterations):
        """
        ğŸ¨ Fonction principale de transfert de style
        
        Analogie : Comme un peintre qui mÃ©lange deux techniques :
        - Il garde la forme et structure de son modÃ¨le (contenu)
        - Il applique la technique d'un maÃ®tre (style)
        
        Le processus est itÃ©ratif, comme un artiste qui amÃ©liore 
        progressivement son Å“uvre coup de pinceau par coup de pinceau.
        
        Args:
            content_path: Image de contenu (ce qu'on veut styliser)
            style_path: Image de style (l'art qu'on veut imiter)
            style_weight: Importance du style artistique
            content_weight: Importance de rester fidÃ¨le au contenu
            iterations: Nombre d'amÃ©liorations Ã  faire
            
        Returns:
            Image stylisÃ©e finale
        """
        # ğŸ“‹ Configuration des couches d'analyse
        content_layers = ['block5_conv2']  # Couche sÃ©mantique profonde
        style_layers = build_style_layers_list()  # Selon sÃ©lection utilisateur
        
        if not style_layers:
            st.error("âŒ Aucune couche de style sÃ©lectionnÃ©e ! Activez au moins une couche.")
            return None

        st.info(f"ğŸ¨ Utilisation de {len(style_layers)} couches de style : {', '.join(style_layers)}")

        # ğŸ—ï¸ CrÃ©ation du modÃ¨le extracteur
        extractor = StyleContentModel(style_layers, content_layers)

        # ğŸ› ï¸ PrÃ©processing des images d'entrÃ©e
        content_image = preprocess_image(content_path, max_dim=max_image_size)
        style_image = preprocess_image(style_path, max_dim=max_image_size)

        # ğŸ” Affichage debug des images prÃ©processÃ©es
        if debug_mode:
            st.write("ğŸ” **VÃ©rification des images prÃ©processÃ©es :**")
            col_debug1, col_debug2 = st.columns(2)
            
            with col_debug1:
                debug_content = deprocess_image(content_image.numpy())
                st.image(debug_content, caption=f"Contenu ({debug_content.shape[1]}Ã—{debug_content.shape[0]})", width=200)
                
            with col_debug2:
                debug_style = deprocess_image(style_image.numpy())
                st.image(debug_style, caption=f"Style ({debug_style.shape[1]}Ã—{debug_style.shape[0]})", width=200)

        # ğŸ¯ Extraction des cibles (ce qu'on veut atteindre)
        # Analogie : Prendre des "mesures" de l'Å“uvre de rÃ©fÃ©rence
        style_targets = extractor(style_image)['style']
        content_targets = extractor(content_image)['content']

        # ğŸ¨ Initialisation de l'image de travail
        # On commence avec l'image de contenu et on la modifie progressivement
        image = tf.Variable(content_image, dtype=tf.float32)
        
        # âš™ï¸ Configuration de l'optimiseur Adam
        # Analogie : RÃ©gler les paramÃ¨tres du "pinceau intelligent"
        opt = tf.optimizers.Adam(
            learning_rate=learning_rate,
            beta_1=beta1,  # MÃ©moire des gradients
            beta_2=beta2,  # MÃ©moire de la variance
            epsilon=epsilon  # StabilitÃ© numÃ©rique
        )

        # ğŸ“Š Interface de progression
        progress_bar = st.progress(0)
        status_text = st.empty()
        loss_placeholder = st.empty()
        preview_placeholder = st.empty()
        
        def train_step(image):
            """
            ğŸ¯ Une Ã©tape d'amÃ©lioration
            
            Analogie : Un coup de pinceau guidÃ© par l'intelligence artificielle.
            L'IA regarde l'image actuelle, calcule ce qui ne va pas,
            et applique une petite correction.
            """
            with tf.GradientTape() as tape:
                # ğŸ” Analyse de l'image actuelle
                outputs = extractor(image)
                
                # âš–ï¸ Calcul de l'erreur (perte)
                loss = style_content_loss(
                    outputs, style_targets, content_targets, 
                    style_weight, content_weight
                )

            # ğŸ“ Calcul des gradients (direction d'amÃ©lioration)
            grad = tape.gradient(loss, image)
            
            # ğŸ¨ Application de la correction
            opt.apply_gradients([(grad, image)])
            
            # ğŸ›¡ï¸ Maintien des valeurs dans [0,1]
            image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0))
            
            return loss

        # ğŸ¨ Boucle principale d'amÃ©lioration artistique
        st.write(f"ğŸ¨ DÃ©but du transfert de style avec {iterations} itÃ©rations...")
        
        best_loss = float('inf')
        best_image = None
        
        for i in range(iterations):
            try:
                # ğŸ¯ Une Ã©tape d'amÃ©lioration
                loss = train_step(image)
                loss_value = float(loss)
                
                # ğŸ“ˆ Suivi du meilleur rÃ©sultat
                if loss_value < best_loss:
                    best_loss = loss_value
                    best_image = image.numpy().copy()
                
                # ğŸ“Š Mise Ã  jour de l'interface
                progress = (i + 1) / iterations
                progress_bar.progress(progress)
                
                # ğŸ“ˆ Affichage dÃ©taillÃ© du statut
                status_text.markdown(f"""
                **ItÃ©ration {i+1}/{iterations}**
                - Perte actuelle: {loss_value:.4f}
                - Meilleure perte: {best_loss:.4f}
                - Progression: {progress:.1%}
                """)
                
                # ğŸ” AperÃ§u pÃ©riodique en mode debug
                if debug_mode and (i + 1) % preview_frequency == 0:
                    preview_img = deprocess_image(image.numpy())
                    with preview_placeholder.container():
                        col1, col2 = st.columns(2)
                        with col1:
                            st.image(preview_img, caption=f"AperÃ§u - ItÃ©ration {i+1}", width=300)
                        with col2:
                            st.metric("Perte", f"{loss_value:.4f}", f"{loss_value - best_loss:.4f}")
                
            except Exception as e:
                st.error(f"âŒ Erreur Ã  l'itÃ©ration {i+1}: {str(e)}")
                break

        # ğŸ§¹ Nettoyage de l'interface
        progress_bar.empty()
        status_text.empty()
        preview_placeholder.empty()
        
        # ğŸ† Retour du meilleur rÃ©sultat trouvÃ©
        return tf.constant(best_image) if best_image is not None else image

    # ğŸ–¼ï¸ Interface utilisateur pour le transfert de style
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### ğŸ“¸ Image de Contenu")
        st.write("*L'image que vous voulez transformer*")
        content_file = st.file_uploader(
            "Choisissez l'image de contenu", 
            type=["jpg", "png", "jpeg"], 
            key="content",
            help="Votre photo personnelle qui sera stylisÃ©e"
        )
        
        if content_file:
            st.image(content_file, caption="Image de contenu", use_column_width=True)
            if show_diagnostics:
                diagnose_image(content_file, "contenu")
    
    with col2:
        st.markdown("### ğŸ¨ Image de Style")
        st.write("*L'Å“uvre d'art dont vous voulez copier le style*")
        style_file = st.file_uploader(
            "Choisissez l'image de style", 
            type=["jpg", "png", "jpeg"], 
            key="style",
            help="Une peinture, dessin ou Å“uvre d'art dont vous aimez le style"
        )
        
        if style_file:
            st.image(style_file, caption="Image de style", use_column_width=True)
            if show_diagnostics:
                diagnose_image(style_file, "style")
    
    # ğŸ¨ Configuration rapide prÃ©dÃ©finie
    st.markdown("### âš¡ Configurations Rapides")
    col_presets1, col_presets2, col_presets3 = st.columns(3)
    
    with col_presets1:
        if st.button("ğŸ–¼ï¸ Portrait Artistique"):
            # Configuration optimale pour portraits
            st.session_state.update({
                'style_weight': 5e3,
                'content_weight': 1e4, 
                'learning_rate': 0.008,
                'iterations': 150
            })
            st.success("Configuration portrait appliquÃ©e !")
    
    with col_presets2:
        if st.button("ğŸï¸ Paysage StylisÃ©"):
            # Configuration optimale pour paysages
            st.session_state.update({
                'style_weight': 8e3,
                'content_weight': 1e3,
                'learning_rate': 0.012, 
                'iterations': 100
            })
            st.success("Configuration paysage appliquÃ©e !")
    
    with col_presets3:
        if st.button("âš¡ Test Rapide"):
            # Configuration pour test rapide
            st.session_state.update({
                'style_weight': 1e4,
                'content_weight': 1e3,
                'learning_rate': 0.02,
                'iterations': 50
            })
            st.success("Configuration test appliquÃ©e !")
    
    # ğŸ“Š PrÃ©diction de temps de calcul
    if content_file and style_file:
        # Estimation basÃ©e sur la taille et les itÃ©rations
        estimated_time = (max_image_size / 512) ** 2 * iterations * 0.05
        st.info(f"â±ï¸ Temps estimÃ© : {estimated_time:.1f} minutes")
        
        if estimated_time > 10:
            st.warning("âš ï¸ Temps long prÃ©vu. ConsidÃ©rez rÃ©duire la taille d'image ou les itÃ©rations.")

    # ğŸš€ Bouton principal de lancement
    if st.button("ğŸ¨ Lancer le Transfert de Style", type="primary", use_container_width=True):
        if content_file and style_file:
            # ğŸ¬ DÃ©but du processus
            start_time = st.empty()
            current_time = datetime.now().strftime('%H:%M:%S')
            start_time.write(f"ğŸš€ **DÃ©marrage du transfert de style Ã  {current_time}**")
            
            with st.spinner('ğŸ¨ Transfert de style en cours... Votre IA crÃ©e une Å“uvre d\'art !'):
                try:
                    process_start = time.time()
                    
                    # ğŸ¨ ExÃ©cution du transfert de style
                    stylized_image = perform_style_transfer(
                        content_file, style_file, 
                        style_weight, content_weight, iterations
                    )
                    
                    if stylized_image is not None:
                        # â±ï¸ Calcul du temps Ã©coulÃ©
                        process_time = time.time() - process_start
                        
                        # ğŸ–¼ï¸ Conversion et affichage du rÃ©sultat
                        result_image = deprocess_image(stylized_image.numpy())
                        
                        # ğŸ¯ Section des rÃ©sultats
                        st.markdown("## ğŸ¯ RÃ©sultat du Transfert de Style")
                        
                        # ğŸ“Š Comparaison avant/aprÃ¨s
                        col_before, col_after = st.columns(2)
                        
                        with col_before:
                            st.markdown("#### ğŸ“¸ Avant (Original)")
                            st.image(content_file, use_column_width=True)
                        
                        with col_after:
                            st.markdown("#### ğŸ¨ AprÃ¨s (StylisÃ©)")
                            st.image(result_image, use_column_width=True)
                        
                        # ğŸ“ˆ Statistiques du processus
                        st.markdown("### ğŸ“Š Statistiques du Processus")
                        col_stats1, col_stats2, col_stats3, col_stats4 = st.columns(4)
                        
                        with col_stats1:
                            st.metric("â±ï¸ Temps", f"{process_time:.1f}s")
                        with col_stats2:
                            st.metric("ğŸ”„ ItÃ©rations", iterations)
                        with col_stats3:
                            st.metric("ğŸ“ Taille", f"{max_image_size}px")
                        with col_stats4:
                            efficiency = iterations / process_time if process_time > 0 else 0
                            st.metric("âš¡ Vitesse", f"{efficiency:.1f} it/s")
                        
                        # ğŸ¨ Informations sur la configuration utilisÃ©e
                        with st.expander("ğŸ”§ Configuration UtilisÃ©e"):
                            st.write(f"**Poids du style :** {style_weight:.0e}")
                            st.write(f"**Poids du contenu :** {content_weight:.0e}")
                            st.write(f"**Taux d'apprentissage :** {learning_rate}")
                            st.write(f"**Couches de style :** {', '.join(build_style_layers_list())}")
                            st.write(f"**Optimiseur Adam :** Î²â‚={beta1}, Î²â‚‚={beta2}, Îµ={epsilon:.0e}")
                        
                        # ğŸ’¾ TÃ©lÃ©chargement du rÃ©sultat
                        result_pil = Image.fromarray(result_image)
                        
                        # ğŸ¨ Options de sauvegarde
                        st.markdown("### ğŸ’¾ TÃ©lÃ©chargement")
                        
                        col_download1, col_download2 = st.columns(2)
                        
                        with col_download1:
                            # PNG haute qualitÃ©
                            buf_png = io.BytesIO()
                            result_pil.save(buf_png, format='PNG', optimize=True)
                            
                            st.download_button(
                                label="ğŸ“¥ TÃ©lÃ©charger PNG (Haute QualitÃ©)",
                                data=buf_png.getvalue(),
                                file_name=f"saffire_stylized_{int(time.time())}.png",
                                mime="image/png",
                                use_container_width=True
                            )
                        
                        with col_download2:
                            # JPEG optimisÃ©
                            buf_jpg = io.BytesIO()
                            result_pil.save(buf_jpg, format='JPEG', quality=95, optimize=True)
                            
                            st.download_button(
                                label="ğŸ“¥ TÃ©lÃ©charger JPEG (OptimisÃ©)",
                                data=buf_jpg.getvalue(),
                                file_name=f"saffire_stylized_{int(time.time())}.jpg",
                                mime="image/jpeg",
                                use_container_width=True
                            )
                        
                        # ğŸ‰ Message de succÃ¨s avec conseils
                        st.success("âœ… Transfert de style terminÃ© avec succÃ¨s !")
                        
                        # ğŸ’¡ Conseils pour amÃ©liorer
                        st.markdown("### ğŸ’¡ Conseils pour amÃ©liorer le rÃ©sultat")
                        st.info("""
                        **Pour plus de style artistique :** Augmentez le poids du style
                        
                        **Pour prÃ©server plus l'original :** Augmentez le poids du contenu
                        
                        **Si l'image semble floue :** Augmentez le nombre d'itÃ©rations
                        
                        **Si les couleurs sont ternes :** Ajustez la saturation dans les paramÃ¨tres avancÃ©s
                        """)
                        
                        # ğŸ“Š Analyse de qualitÃ© automatique
                        st.markdown("### ğŸ” Analyse de QualitÃ©")
                        
                        # Calculs simples de qualitÃ©
                        original_array = np.array(Image.open(content_file).resize((256, 256)))
                        result_resized = np.array(result_pil.resize((256, 256)))
                        
                        # Mesure de similaritÃ© (simple MSE)
                        mse = np.mean((original_array.astype(float) - result_resized.astype(float)) ** 2)
                        similarity = max(0, 100 - mse / 100)  # Score approximatif
                        
                        col_quality1, col_quality2 = st.columns(2)
                        with col_quality1:
                            st.metric("ğŸ¯ SimilaritÃ© contenu", f"{similarity:.1f}%")
                        with col_quality2:
                            color_variance = np.var(result_resized)
                            st.metric("ğŸŒˆ Richesse couleurs", f"{color_variance:.0f}")
                    
                    else:
                        st.error("âŒ Erreur lors du transfert de style.")
                        
                except Exception as e:
                    st.error(f"âŒ Erreur lors du transfert de style : {str(e)}")
                    
                    # ğŸ”§ Suggestions de rÃ©solution
                    st.markdown("### ğŸ”§ Suggestions de rÃ©solution :")
                    st.write("1. VÃ©rifiez que vos images sont au format JPG/PNG")
                    st.write("2. Essayez de rÃ©duire la taille maximale d'image") 
                    st.write("3. RÃ©duisez le nombre d'itÃ©rations pour un test")
                    st.write("4. VÃ©rifiez qu'au moins une couche de style est activÃ©e")
                    
        else:
            st.warning("âš ï¸ Veuillez charger une image de contenu ET une image de style.")

# ğŸ”„ MODULE DE TRANSFORMATION INVERSE
elif main_mode == "Transformation Inverse":
    st.markdown("## ğŸ”„ Module de Transformation Inverse")
    st.write("*RÃ©cupÃ©rez le contenu original ou extrayez le style d'une image stylisÃ©e*")
    
    # ğŸ›ï¸ HyperparamÃ¨tres pour la transformation inverse
    st.sidebar.markdown("### ğŸ”„ ParamÃ¨tres de Transformation Inverse")
    
    # Type de transformation inverse
    inverse_mode = st.sidebar.radio(
        "Type de transformation:",
        ["Extraction de Contenu", "Extraction de Style", "DÃ©stylisation ComplÃ¨te"],
        help="Choisissez quel aspect rÃ©cupÃ©rer de l'image stylisÃ©e"
    )
    
    # IntensitÃ© de la transformation inverse
    inverse_strength = st.sidebar.slider(
        "IntensitÃ© de rÃ©cupÃ©ration:",
        0.1, 2.0, 1.0, step=0.1,
        help="Plus Ã©levÃ© = rÃ©cupÃ©ration plus agressive"
    )
    
    # Nombre d'itÃ©rations pour l'optimisation inverse
    inverse_iterations = st.sidebar.number_input(
        "ItÃ©rations d'optimisation:",
        min_value=50, max_value=500, value=200, step=25,
        help="Plus d'itÃ©rations = meilleure qualitÃ© mais plus lent"
    )
    
    # ParamÃ¨tres avancÃ©s
    st.sidebar.markdown("### ğŸ”§ ParamÃ¨tres AvancÃ©s")
    
    inverse_learning_rate = st.sidebar.slider(
        "Taux d'apprentissage inverse:",
        0.001, 0.05, 0.01, step=0.001,
        help="Vitesse de rÃ©cupÃ©ration - plus lent mais plus stable"
    )
    
    content_preservation = st.sidebar.slider(
        "PrÃ©servation structure:",
        0.0, 2.0, 1.0, step=0.1,
        help="Force de prÃ©servation de la structure originale"
    )
    
    # RÃ©gularisation pour Ã©viter les artefacts
    regularization_weight = st.sidebar.slider(
        "RÃ©gularisation (anti-artefacts):",
        0.0, 0.1, 0.01, step=0.005,
        help="Ã‰vite les pixels aberrants et lisse le rÃ©sultat"
    )
    
    # Type de perte pour l'optimisation inverse
    loss_type = st.sidebar.selectbox(
        "Type de perte d'optimisation:",
        ["MSE", "Perceptual", "Mixed"],
        help="MSE=simple | Perceptual=rÃ©aliste | Mixed=Ã©quilibrÃ©"
    )
    
    # Options de post-traitement
    st.sidebar.markdown("#### ğŸ¨ Post-traitement")
    
    enhance_details = st.sidebar.checkbox(
        "AmÃ©lioration des dÃ©tails",
        value=True,
        help="Renforce les contours et textures rÃ©cupÃ©rÃ©s"
    )
    
    noise_reduction = st.sidebar.slider(
        "RÃ©duction du bruit:",
        0.0, 1.0, 0.3, step=0.1,
        help="Lisse les artefacts de reconstruction"
    )
    
    color_correction = st.sidebar.checkbox(
        "Correction colorimÃ©trique",
        value=True,
        help="Ajuste automatiquement les couleurs rÃ©cupÃ©rÃ©es"
    )

    def create_inverse_model(target_size=(512, 512)):
        """
        ğŸ”„ CrÃ©ation du modÃ¨le de transformation inverse
        
        Analogie : Comme un "dÃ©tective artistique" qui analyse une Å“uvre
        pour retrouver les Ã©lÃ©ments originaux cachÃ©s dessous.
        
        Le modÃ¨le utilise un autoencoder avec skip connections pour
        reconstruire le contenu ou style original.
        
        Args:
            target_size: Taille de l'image de sortie
            
        Returns:
            ModÃ¨le TensorFlow pour transformation inverse
        """
        from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Concatenate, UpSampling2D
        from tensorflow.keras.layers import LeakyReLU, BatchNormalization
        
        # ğŸ“¥ EntrÃ©e : Image stylisÃ©e
        inputs = Input(shape=(*target_size, 3), name='stylized_input')
        
        # ğŸ”½ Encodeur - "Analyse de l'image stylisÃ©e"
        # Analogie : DÃ©composer l'image en Ã©lÃ©ments comprÃ©hensibles
        
        # Block 1: Extraction des features de base
        e1 = Conv2D(64, 3, padding='same', name='encoder_1')(inputs)
        e1 = LeakyReLU(alpha=0.2)(e1)
        e1 = BatchNormalization()(e1)
        
        # Block 2: Features intermÃ©diaires
        e2 = Conv2D(128, 3, strides=2, padding='same', name='encoder_2')(e1)
        e2 = LeakyReLU(alpha=0.2)(e2)
        e2 = BatchNormalization()(e2)
        
        # Block 3: Features profondes
        e3 = Conv2D(256, 3, strides=2, padding='same', name='encoder_3')(e2)
        e3 = LeakyReLU(alpha=0.2)(e3)
        e3 = BatchNormalization()(e3)
        
        # Block 4: ReprÃ©sentation latente
        e4 = Conv2D(512, 3, strides=2, padding='same', name='encoder_4')(e3)
        e4 = LeakyReLU(alpha=0.2)(e4)
        e4 = BatchNormalization()(e4)
        
        # ğŸ”¼ DÃ©codeur - "Reconstruction du contenu original"
        # Analogie : Remonter du puzzle dÃ©composÃ© vers l'image originale
        
        # Block 1: DÃ©but de reconstruction
        d1 = Conv2DTranspose(256, 3, strides=2, padding='same', name='decoder_1')(e4)
        d1 = LeakyReLU(alpha=0.2)(d1)
        d1 = BatchNormalization()(d1)
        d1 = Concatenate()([d1, e3])  # Skip connection pour prÃ©server les dÃ©tails
        
        # Block 2: Reconstruction intermÃ©diaire
        d2 = Conv2DTranspose(128, 3, strides=2, padding='same', name='decoder_2')(d1)
        d2 = LeakyReLU(alpha=0.2)(d2)
        d2 = BatchNormalization()(d2)
        d2 = Concatenate()([d2, e2])  # Skip connection
        
        # Block 3: Reconstruction finale
        d3 = Conv2DTranspose(64, 3, strides=2, padding='same', name='decoder_3')(d2)
        d3 = LeakyReLU(alpha=0.2)(d3)
        d3 = BatchNormalization()(d3)
        d3 = Concatenate()([d3, e1])  # Skip connection
        
        # ğŸ¯ Sortie finale
        outputs = Conv2D(3, 3, activation='tanh', padding='same', name='output')(d3)
        
        # CrÃ©ation du modÃ¨le
        model = tf.keras.Model(inputs=inputs, outputs=outputs, name='InverseTransformModel')
        
        return model

    def perceptual_loss(y_true, y_pred, vgg_model):
        """
        ğŸ‘ï¸ Calcul de la perte perceptuelle
        
        Analogie : Au lieu de comparer pixel par pixel (comme un robot),
        on compare ce que "voit" un expert (rÃ©seau VGG19 prÃ©-entraÃ®nÃ©).
        
        Args:
            y_true: Image cible
            y_pred: Image prÃ©dite
            vgg_model: ModÃ¨le VGG19 pour extraction de features
            
        Returns:
            Perte perceptuelle basÃ©e sur les features VGG19
        """
        # PrÃ©processing pour VGG19
        y_true_vgg = preprocess_input(y_true * 255.0)
        y_pred_vgg = preprocess_input(y_pred * 255.0)
        
        # Extraction des features
        true_features = vgg_model(y_true_vgg)
        pred_features = vgg_model(y_pred_vgg)
        
        # Calcul de la diffÃ©rence perceptuelle
        loss = 0
        for true_feat, pred_feat in zip(true_features, pred_features):
            loss += tf.reduce_mean(tf.square(true_feat - pred_feat))
        
        return loss

    def total_variation_loss(image):
        """
        ğŸŒŠ Perte de variation totale pour rÃ©duction du bruit
        
        Analogie : Comme lisser une surface rugueuse pour la rendre plus naturelle.
        Cette fonction pÃ©nalise les variations brutales entre pixels voisins.
        
        Args:
            image: Image Ã  lisser
            
        Returns:
            Perte de variation totale
        """
        # DiffÃ©rences horizontales et verticales
        h_diff = image[:, 1:, :, :] - image[:, :-1, :, :]
        w_diff = image[:, :, 1:, :] - image[:, :, :-1, :]
        
        # Somme des variations
        return tf.reduce_mean(tf.square(h_diff)) + tf.reduce_mean(tf.square(w_diff))

    def perform_inverse_transform(stylized_image, reference_image=None):
        """
        ğŸ”„ ExÃ©cution de la transformation inverse
        
        Analogie : Comme un restaurateur d'art qui enlÃ¨ve les couches
        de peinture ajoutÃ©es pour retrouver l'Å“uvre originale en dessous.
        
        Args:
            stylized_image: Image stylisÃ©e Ã  transformer
            reference_image: Image de rÃ©fÃ©rence (optionnelle)
            
        Returns:
            Image avec transformation inverse appliquÃ©e
        """
        # ğŸ—ï¸ PrÃ©paration du modÃ¨le VGG19 pour perte perceptuelle
        vgg = VGG19(include_top=False, weights='imagenet')
        vgg.trainable = False
        
        # SÃ©lection des couches pour perte perceptuelle
        feature_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3']
        feature_outputs = [vgg.get_layer(name).output for name in feature_layers]
        feature_model = tf.keras.Model([vgg.input], feature_outputs)
        
        # ğŸ¯ Initialisation de l'image de travail
        # On commence avec l'image stylisÃ©e et on la modifie progressivement
        target_image = tf.Variable(stylized_image, dtype=tf.float32)
        
        # âš™ï¸ Optimiseur pour la transformation inverse
        optimizer = tf.optimizers.Adam(learning_rate=inverse_learning_rate)
        
        # ğŸ“Š Interface de progression
        progress_bar = st.progress(0)
        status_text = st.empty()
        preview_placeholder = st.empty()
        
        @tf.function
        def inverse_step():
            """
            ğŸ”„ Une Ã©tape d'optimisation inverse
            
            Calcule et applique une correction pour se rapprocher
            de l'objectif de transformation inverse.
            """
            with tf.GradientTape() as tape:
                # ğŸ“Š Calcul des diffÃ©rentes pertes
                total_loss = 0
                
                if loss_type in ["MSE", "Mixed"]:
                    # ğŸ“ Perte MSE simple (pixel par pixel)
                    if reference_image is not None:
                        mse_loss = tf.reduce_mean(tf.square(target_image - reference_image))
                        total_loss += mse_loss * inverse_strength
                
                if loss_type in ["Perceptual", "Mixed"]:
                    # ğŸ‘ï¸ Perte perceptuelle (basÃ©e sur la vision)
                    if reference_image is not None:
                        perc_loss = perceptual_loss(reference_image, target_image, feature_model)
                        total_loss += perc_loss * inverse_strength * 0.1
                
                # ğŸ›¡ï¸ RÃ©gularisation pour Ã©viter les artefacts
                if regularization_weight > 0:
                    tv_loss = total_variation_loss(target_image)
                    total_loss += tv_loss * regularization_weight
                
                # ğŸ—ï¸ PrÃ©servation de la structure si demandÃ©e
                if content_preservation > 0:
                    structure_loss = tf.reduce_mean(tf.square(
                        tf.image.sobel_edges(target_image) - 
                        tf.image.sobel_edges(stylized_image)
                    ))
                    total_loss += structure_loss * content_preservation
            
            # ğŸ“ Calcul et application des gradients
            gradients = tape.gradient(total_loss, target_image)
            optimizer.apply_gradients([(gradients, target_image)])
            
            # ğŸ›¡ï¸ Maintien des valeurs dans [0,1]
            target_image.assign(tf.clip_by_value(target_image, 0.0, 1.0))
            
            return total_loss
        
        # ğŸ”„ Boucle d'optimisation inverse
        st.write(f"ğŸ”„ DÃ©but de la transformation inverse ({inverse_mode})...")
        
        best_loss = float('inf')
        best_image = None
        
        for i in range(inverse_iterations):
            try:
                # ğŸ¯ Une Ã©tape d'amÃ©lioration
                loss = inverse_step()
                loss_value = float(loss)
                
                # ğŸ“ˆ Suivi du meilleur rÃ©sultat
                if loss_value < best_loss:
                    best_loss = loss_value
                    best_image = target_image.numpy().copy()
                
                # ğŸ“Š Mise Ã  jour de l'interface
                progress = (i + 1) / inverse_iterations
                progress_bar.progress(progress)
                
                status_text.markdown(f"""
                **ItÃ©ration {i+1}/{inverse_iterations}**
                - Perte: {loss_value:.6f}
                - Meilleure: {best_loss:.6f}
                - Mode: {inverse_mode}
                """)
                
                # ğŸ” AperÃ§u pÃ©riodique
                if (i + 1) % 25 == 0:
                    preview_img = deprocess_image(target_image.numpy())
                    with preview_placeholder.container():
                        st.image(preview_img, caption=f"Progression - ItÃ©ration {i+1}", width=300)
                
            except Exception as e:
                st.error(f"âŒ Erreur Ã  l'itÃ©ration {i+1}: {str(e)}")
                break
        
        # ğŸ§¹ Nettoyage interface
        progress_bar.empty()
        status_text.empty()
        preview_placeholder.empty()
        
        # ğŸ¨ Post-traitement si demandÃ©
        final_image = tf.constant(best_image) if best_image is not None else target_image
        
        if enhance_details:
            # ğŸ” AmÃ©lioration des dÃ©tails via filtre passe-haut
            kernel = tf.constant([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype=tf.float32)
            kernel = tf.reshape(kernel, [3, 3, 1, 1])
            kernel = tf.tile(kernel, [1, 1, 3, 1])  # Pour les 3 canaux RGB
            
            details = tf.nn.conv2d(final_image, kernel, strides=[1, 1, 1, 1], padding='SAME')
            final_image = final_image + details * 0.1  # Ajout subtil des dÃ©tails
        
        if noise_reduction > 0:
            # ğŸŒŠ RÃ©duction du bruit par filtrage gaussien
            final_image = tf.image.gaussian_filter2d(final_image, sigma=noise_reduction)
        
        # ğŸ›¡ï¸ Clipping final
        final_image = tf.clip_by_value(final_image, 0.0, 1.0)
        
        return final_image

    # ğŸ–¼ï¸ Interface utilisateur
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### ğŸ¨ Image StylisÃ©e")
        st.write("*L'image stylisÃ©e dont vous voulez extraire des Ã©lÃ©ments*")
        stylized_file = st.file_uploader(
            "Choisissez l'image stylisÃ©e", 
            type=["jpg", "png", "jpeg"], 
            key="stylized",
            help="L'image qui a subi un transfert de style"
        )
        
        if stylized_file:
            st.image(stylized_file, caption="Image stylisÃ©e", use_column_width=True)
    
    with col2:
        st.markdown("### ğŸ“¸ Image de RÃ©fÃ©rence (Optionnelle)")
        st.write("*L'image originale pour guider la transformation inverse*")
        reference_file = st.file_uploader(
            "Choisissez l'image de rÃ©fÃ©rence", 
            type=["jpg", "png", "jpeg"], 
            key="reference",
            help="L'image originale avant stylisation (optionnel)"
        )
        
        if reference_file:
            st.image(reference_file, caption="Image de rÃ©fÃ©rence", use_column_width=True)
    
    # â„¹ï¸ Explication du mode sÃ©lectionnÃ©
    if inverse_mode == "Extraction de Contenu":
        st.info("ğŸ¯ **Mode Extraction de Contenu** : RÃ©cupÃ¨re les formes et structures originales en supprimant les effets de style")
    elif inverse_mode == "Extraction de Style":
        st.info("ğŸ¨ **Mode Extraction de Style** : Isole les Ã©lÃ©ments stylistiques (textures, coups de pinceau) pour les rÃ©utiliser")
    else:
        st.info("ğŸ”„ **Mode DÃ©stylisation ComplÃ¨te** : Tente de retrouver l'image originale complÃ¨te avant stylisation")
    
    # ğŸš€ Bouton de lancement
    if st.button("ğŸ”„ Lancer la Transformation Inverse", type="primary", use_container_width=True):
        if stylized_file:
            with st.spinner(f'ğŸ”„ Transformation inverse en cours ({inverse_mode})...'):
                try:
                    start_time = time.time()
                    
                    # ğŸ› ï¸ PrÃ©paration des images
                    stylized_image = preprocess_image(stylized_file, max_dim=512)
                    reference_image = None
                    
                    if reference_file:
                        reference_image = preprocess_image(reference_file, max_dim=512)
                        # Redimensionner pour correspondre Ã  l'image stylisÃ©e
                        ref_shape = tf.shape(reference_image)
                        sty_shape = tf.shape(stylized_image)
                        if ref_shape[1] != sty_shape[1] or ref_shape[2] != sty_shape[2]:
                            reference_image = tf.image.resize(reference_image, [sty_shape[1], sty_shape[2]])
                    
                    # ğŸ”„ ExÃ©cution de la transformation inverse
                    result_image = perform_inverse_transform(stylized_image, reference_image)
                    
                    # â±ï¸ Calcul du temps
                    process_time = time.time() - start_time
                    
                    # ğŸ–¼ï¸ Affichage des rÃ©sultats
                    result_array = deprocess_image(result_image.numpy())
                    
                    st.markdown("## ğŸ¯ RÃ©sultat de la Transformation Inverse")
                    
                    # ğŸ“Š Comparaison avant/aprÃ¨s
                    if reference_file:
                        col_original, col_stylized, col_recovered = st.columns(3)
                        
                        with col_original:
                            st.markdown("#### ğŸ“¸ Original")
                            st.image(reference_file, use_column_width=True)
                        
                        with col_stylized:
                            st.markdown("#### ğŸ¨ StylisÃ©")
                            st.image(stylized_file, use_column_width=True)
                        
                        with col_recovered:
                            st.markdown("#### ğŸ”„ RÃ©cupÃ©rÃ©")
                            st.image(result_array, use_column_width=True)
                    else:
                        col_before, col_after = st.columns(2)
                        
                        with col_before:
                            st.markdown("#### ğŸ¨ Avant (StylisÃ©)")
                            st.image(stylized_file, use_column_width=True)
                        
                        with col_after:
                            st.markdown("#### ğŸ”„ AprÃ¨s (TransformÃ©)")
                            st.image(result_array, use_column_width=True)
                    
                    # ğŸ“ˆ Statistiques
                    st.markdown("### ğŸ“Š Statistiques du Processus")
                    col_stats1, col_stats2, col_stats3 = st.columns(3)
                    
                    with col_stats1:
                        st.metric("â±ï¸ Temps", f"{process_time:.1f}s")
                    with col_stats2:
                        st.metric("ğŸ”„ ItÃ©rations", inverse_iterations)
                    with col_stats3:
                        efficiency = inverse_iterations / process_time if process_time > 0 else 0
                        st.metric("âš¡ Vitesse", f"{efficiency:.1f} it/s")
                    
                    # ğŸ’¾ TÃ©lÃ©chargement
                    result_pil = Image.fromarray(result_array)
                    buf = io.BytesIO()
                    result_pil.save(buf, format='PNG')
                    
                    st.download_button(
                        label="ğŸ“¥ TÃ©lÃ©charger le RÃ©sultat",
                        data=buf.getvalue(),
                        file_name=f"saffire_inverse_{inverse_mode.lower().replace(' ', '_')}_{int(time.time())}.png",
                        mime="image/png",
                        use_container_width=True
                    )
                    
                    # âœ… Message de succÃ¨s
                    st.success(f"âœ… Transformation inverse ({inverse_mode}) terminÃ©e avec succÃ¨s !")
                    
                    # ğŸ’¡ Conseils d'amÃ©lioration
                    st.markdown("### ğŸ’¡ Conseils pour AmÃ©liorer")
                    if inverse_mode == "Extraction de Contenu":
                        st.info("ğŸ’¡ Si le contenu n'est pas assez rÃ©cupÃ©rÃ©, augmentez l'intensitÃ© de rÃ©cupÃ©ration ou utilisez une image de rÃ©fÃ©rence")
                    elif inverse_mode == "Extraction de Style":
                        st.info("ğŸ’¡ Pour isoler mieux le style, essayez de rÃ©duire la prÃ©servation de structure")
                    else:
                        st.info("ğŸ’¡ Pour une meilleure dÃ©stylisation, fournissez l'image originale comme rÃ©fÃ©rence")
                
                except Exception as e:
                    st.error(f"âŒ Erreur lors de la transformation inverse : {str(e)}")
                    st.write("ğŸ”§ **Solutions possibles :**")
                    st.write("- VÃ©rifiez le format de votre image (JPG/PNG)")
                    st.write("- RÃ©duisez le nombre d'itÃ©rations pour un test")
                    st.write("- Essayez avec une image plus petite")
        else:
            st.warning("âš ï¸ Veuillez charger au minimum une image stylisÃ©e.")
    
    # ğŸ“š Section d'information
    with st.expander("â„¹ï¸ Comment fonctionne la Transformation Inverse ?"):
        st.markdown("""
        ### ğŸ”„ Principe de la Transformation Inverse
        
        La transformation inverse tente de "dÃ©faire" les effets du transfert de style pour rÃ©cupÃ©rer 
        les Ã©lÃ©ments originaux cachÃ©s dans l'image stylisÃ©e.
        
        ### ğŸ§  Processus Technique
        
        **1. Analyse de l'Image StylisÃ©e** ğŸ”
        - DÃ©composition en features via un rÃ©seau encodeur-dÃ©codeur
        - Identification des Ã©lÃ©ments de contenu vs style
        - SÃ©paration des composantes visuelles
        
        **2. Optimisation Inverse** âš™ï¸
        - Utilisation de gradients pour "remonter le temps"
        - Minimisation de la diffÃ©rence avec l'objectif
        - RÃ©gularisation pour Ã©viter les artefacts
        
        **3. Reconstruction** ğŸ—ï¸
        - Assemblage des Ã©lÃ©ments rÃ©cupÃ©rÃ©s
        - Post-traitement pour amÃ©liorer la qualitÃ©
        - Lissage et correction des couleurs
        
        ### ğŸ¯ Modes de Transformation
        
        **Extraction de Contenu** ğŸ“¸
        - RÃ©cupÃ¨re les formes et structures
        - Supprime les textures artistiques
        - IdÃ©al pour retrouver la gÃ©omÃ©trie originale
        
        **Extraction de Style** ğŸ¨
        - Isole les Ã©lÃ©ments stylistiques
        - Garde les textures et coups de pinceau
        - Utile pour crÃ©er des templates de style
        
        **DÃ©stylisation ComplÃ¨te** ğŸ”„
        - Tente de retrouver l'image originale
        - Combine rÃ©cupÃ©ration de contenu et suppression de style
        - Meilleur rÃ©sultat avec image de rÃ©fÃ©rence
        
        ### âš™ï¸ ParamÃ¨tres ClÃ©s
        
        **IntensitÃ© de RÃ©cupÃ©ration** ğŸ’ª
        - ContrÃ´le la force de la transformation inverse
        - Plus Ã©levÃ© = rÃ©cupÃ©ration plus agressive
        - Risque : artefacts si trop Ã©levÃ©
        
        **PrÃ©servation Structure** ğŸ—ï¸
        - Maintient la gÃ©omÃ©trie de base
        - Important pour l'extraction de contenu
        - Ã‰vite les dÃ©formations excessives
        
        **RÃ©gularisation** ğŸ›¡ï¸
        - Ã‰vite les pixels aberrants
        - Lisse le rÃ©sultat final
        - Ã‰quilibrer avec la qualitÃ© des dÃ©tails
        
        ### ğŸ’¡ Conseils d'Utilisation
        
        **Pour de Meilleurs RÃ©sultats** âœ¨
        - Utilisez l'image originale comme rÃ©fÃ©rence si disponible
        - Commencez avec des paramÃ¨tres conservateurs
        - Augmentez progressivement l'intensitÃ©
        - Testez diffÃ©rents modes selon votre objectif
        
        **Limitations** âš ï¸
        - La transformation inverse n'est jamais parfaite
        - Certaines informations sont dÃ©finitivement perdues
        - La qualitÃ© dÃ©pend du niveau de stylisation initial
        - Plus l'image Ã©tait stylisÃ©e, plus difficile la rÃ©cupÃ©ration
        """)

# ğŸ“š Section d'information dÃ©taillÃ©e
    with st.expander("â„¹ï¸ Comment fonctionne le Transfert de Style Neural ?"):
        st.markdown("""
        ### ğŸ§  Principe de Base
        
        Le transfert de style neural utilise l'intelligence artificielle pour **sÃ©parer** et **recombiner** 
        deux aspects d'une image :
        
        1. **Le Contenu** ğŸ“¸ : La structure, les formes, les objets (QUOI est dans l'image)
        2. **Le Style** ğŸ¨ : Les textures, couleurs, coups de pinceau (COMMENT c'est peint)
        
        ### ğŸ”¬ Le Processus Technique
        
        **Ã‰tape 1 - Analyse** ğŸ”
        - L'IA "regarde" votre photo avec les "yeux" d'un rÃ©seau VGG19 prÃ©-entraÃ®nÃ©
        - Elle identifie les formes et objets (contenu) dans les couches profondes
        - Elle analyse les textures et patterns (style) dans plusieurs couches
        
        **Ã‰tape 2 - Extraction des "Signatures"** ğŸ“Š
        - **Contenu** : Features maps de la couche block5_conv2 (comprÃ©hension sÃ©mantique)
        - **Style** : Matrices de Gram des couches block1 Ã  block5 (corrÃ©lations de textures)
        
        **Ã‰tape 3 - Optimisation ItÃ©rative** ğŸ¯
        - L'IA commence avec votre photo originale
        - Ã€ chaque itÃ©ration, elle la modifie lÃ©gÃ¨rement pour :
          - Garder le mÃªme contenu (fidÃ©litÃ© Ã  l'original)
          - Adopter le style de l'Å“uvre d'art (transformation artistique)
        - Le processus s'arrÃªte quand l'Ã©quilibre optimal est trouvÃ©
        
        ### âš–ï¸ Les HyperparamÃ¨tres ExpliquÃ©s
        
        **Poids du Style vs Contenu** ğŸšï¸
        - **Style Ã©levÃ©** â†’ Plus artistique, moins ressemblant
        - **Contenu Ã©levÃ©** â†’ Plus fidÃ¨le, moins stylisÃ©
        - **Ã‰quilibre** â†’ Transformation harmonieuse
        
        **Nombre d'ItÃ©rations** ğŸ”„
        - Comme un peintre qui affine son Å“uvre
        - Plus d'itÃ©rations = meilleur rÃ©sultat mais plus lent
        - 50-100 pour test, 200-500 pour qualitÃ© finale
        
        **Taux d'Apprentissage** âš¡
        - Vitesse des "coups de pinceau" de l'IA
        - Trop rapide â†’ instable, trop lent â†’ convergence lente
        - 0.01 est gÃ©nÃ©ralement optimal
        
        ### ğŸ¨ Conseils d'Utilisation
        
        **Choix des Images** ğŸ“¸
        - **Contenu** : Photos nettes, bien contrastÃ©es
        - **Style** : Å’uvres d'art avec textures riches (Van Gogh, Picasso, etc.)
        
        **Premiers Tests** âš¡
        - Commencez avec la configuration "Test Rapide"
        - Ajustez selon le rÃ©sultat obtenu
        - ExpÃ©rimentez avec diffÃ©rents styles
        
        **Optimisation** ğŸ¯
        - Portrait â†’ PrivilÃ©gier le contenu
        - Paysage â†’ Ã‰quilibrer style/contenu  
        - Art abstrait â†’ PrivilÃ©gier le style
        """)
    
    # ğŸ­ Galerie d'exemples (si vous voulez ajouter des exemples)
    with st.expander("ğŸ–¼ï¸ Galerie d'Exemples et Inspirations"):
        st.markdown("""
        ### ğŸ¨ Styles Artistiques Populaires
        
        **Impressionnisme** ğŸŒ…
        - Van Gogh, Monet, Renoir
        - Effet : Coups de pinceau visibles, couleurs vives
        - IdÃ©al pour : Paysages, portraits
        
        **Cubisme** ğŸ”·
        - Picasso, Braque
        - Effet : Formes gÃ©omÃ©triques, perspectives multiples
        - IdÃ©al pour : Portraits, objets
        
        **Art Japonais** ğŸ—¾
        - Hokusai, style manga
        - Effet : Lignes nettes, couleurs plates
        - IdÃ©al pour : Tous types d'images
        
        **Art Moderne** ğŸ­
        - Kandinsky, Mondrian
        - Effet : Abstraction, couleurs pures
        - IdÃ©al pour : CrÃ©ations artistiques audacieuses
        
        ### ğŸ’¡ Astuces de Pro
        
        1. **Testez diffÃ©rents ratios style/contenu** pour le mÃªme couple d'images
        2. **Utilisez des styles contrastÃ©s** avec votre photo pour des effets saisissants
        3. **Les Å“uvres avec textures prononcÃ©es** donnent de meilleurs rÃ©sultats
        4. **Combinez plusieurs passes** : style lÃ©ger puis style prononcÃ©
        5. **Post-traitez** : ajustez luminositÃ©/contraste aprÃ¨s le transfert
        """)
    
    # ğŸ”§ Section de dÃ©pannage
    with st.expander("ğŸ› ï¸ DÃ©pannage et RÃ©solution de ProblÃ¨mes"):
        st.markdown("""
        ### âŒ ProblÃ¨mes Courants
        
        **"L'image reste floue ou dÃ©formÃ©e"** ğŸŒ«ï¸
        - **Cause** : Trop d'itÃ©rations ou learning rate trop Ã©levÃ©
        - **Solution** : RÃ©duire les itÃ©rations Ã  50-100, learning rate Ã  0.005
        
        **"Le style ne s'applique pas assez"** ğŸ¨
        - **Cause** : Poids du style trop faible
        - **Solution** : Augmenter le poids du style Ã  1e5 ou plus
        
        **"L'image originale disparaÃ®t complÃ¨tement"** ğŸ“¸
        - **Cause** : Poids du contenu trop faible
        - **Solution** : Augmenter le poids du contenu Ã  1e4 ou plus
        
        **"Le processus est trÃ¨s lent"** â³
        - **Cause** : Image trop grande ou trop d'itÃ©rations
        - **Solution** : RÃ©duire taille Ã  256px, limiter Ã  50-100 itÃ©rations
        
        **"Erreur de mÃ©moire"** ğŸ’¾
        - **Cause** : Image trop grande pour votre systÃ¨me
        - **Solution** : Utiliser 256px maximum, redÃ©marrer l'application
        
        **"Couleurs Ã©tranges ou saturÃ©es"** ğŸŒˆ
        - **Cause** : ProblÃ¨me de normalisation ou contraste
        - **Solution** : Ajuster saturation et contraste dans paramÃ¨tres avancÃ©s
        
        ### ğŸ” Diagnostic Auto
        
        Activez le **"Diagnostic dÃ©taillÃ©"** pour voir :
        - Format et qualitÃ© de vos images
        - ProblÃ¨mes potentiels dÃ©tectÃ©s
        - Suggestions d'optimisation automatiques
        
        ### ğŸš€ Optimisation Performance
        
        **Pour des rÃ©sultats plus rapides :**
        - Taille 256px, 50 itÃ©rations
        - DÃ©sactiver le mode dÃ©bogage
        - Utiliser moins de couches de style
        
        **Pour la meilleure qualitÃ© :**
        - Taille 512px minimum
        - 200-500 itÃ©rations
        - Toutes les couches de style activÃ©es
        - Learning rate rÃ©duit (0.005)
        """)
    
    # ğŸ“Š Panneau de monitoring avancÃ© (si debug activÃ©)
    if debug_mode and content_file and style_file:
        st.markdown("### ğŸ”¬ Monitoring AvancÃ© (Mode Debug)")
        
        # Informations systÃ¨me
        col_sys1, col_sys2, col_sys3 = st.columns(3)
        
        with col_sys1:
            st.metric("ğŸ–¥ï¸ Backend", "TensorFlow")
        with col_sys2:
            st.metric("ğŸ§  ModÃ¨le", "VGG19 ImageNet")
        with col_sys3:
            st.metric("âš¡ Device", "CPU" if not tf.config.list_physical_devices('GPU') else "GPU")

# ğŸ“ Footer avec informations
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: gray; padding: 20px;'>
    <p><strong>SAFFIRE Detection System</strong> - Powered by TensorFlow & Streamlit</p>
    <p>ğŸ¨ Module Classification: DÃ©tection intelligente de feu et fumÃ©e</p>
    <p>ğŸ–¼ï¸ Module Style Transfer: Transformation artistique par IA</p>
    <p><em>DÃ©veloppÃ© pour allier sÃ©curitÃ© et crÃ©ativitÃ©</em></p>
</div>
""", unsafe_allow_html=True)