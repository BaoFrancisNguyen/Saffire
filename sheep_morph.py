import streamlit as st
import base64
import zipfile
import os
import shutil
from tensorflow.keras.models import Sequential, load_model, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D
from tensorflow.keras.regularizers import l2
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from tensorflow.keras.applications import VGG19
from tensorflow.keras.applications.vgg19 import preprocess_input
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import tensorflow as tf
from PIL import Image
import io
import time
from datetime import datetime

# üåç Variables globales - Comme des "bo√Ætes de rangement" partag√©es dans toute l'application
model = None  # Notre mod√®le de classification - comme un cerveau entra√Æn√©
model_path = "saved_model.h5"  # Adresse o√π sauvegarder notre cerveau
class_names = []  # Liste des noms de classes - comme un dictionnaire

# üé® Configuration de la page Streamlit - D√©coration de notre interface
st.set_page_config(page_title="SHEEP MORPH - photo style morphing", layout="wide")

# üè∑Ô∏è Interface utilisateur - Titres et pr√©sentation
st.markdown(
    """
    <h1 style="text-align: center; color: black;">SAFFIRE morphing</h1>
    """,
    unsafe_allow_html=True
)

st.markdown(
    """
    <h2 style="text-align: center; color: black;">Powered by AI and TensorFlow</h2>
    """,
    unsafe_allow_html=True
)

st.markdown(
    """
    <p style="text-align: center; color: black;">Changez le style de vos photos</p>
    """,
    unsafe_allow_html=True
)

# üñºÔ∏è Configuration de l'image de fond - Comme changer le papier peint
background_image_path = "background.jpg"

if os.path.exists(background_image_path):
    with open(background_image_path, "rb") as image_file:
        # Conversion de l'image en format web (Base64) - comme traduire une langue
        encoded_string = base64.b64encode(image_file.read()).decode()
    
    # Injection du CSS pour l'arri√®re-plan - comme peindre les murs
    st.markdown(
        f"""
        <style>
        .stApp {{
            background-image: url(data:image/png;base64,{encoded_string});
            background-size: cover;
            background-repeat: no-repeat;
            background-attachment: fixed;
        }}
        </style>
        """,
        unsafe_allow_html=True
    )

# üéõÔ∏è Interface de contr√¥le principale - Menu de navigation
st.sidebar.header("Configuration")
main_mode = st.sidebar.radio("S√©lectionnez le module:", ["Classification", "Transfert de Style"])

# üìä MODULE DE CLASSIFICATION
if main_mode == "Classification":
    mode = st.sidebar.radio("Select Mode:", ["Automatic", "Manual"])
    
    # Logo dans la barre lat√©rale - D√©coration
    if os.path.exists("logo.jpg"):
        st.sidebar.image("logo.jpg", width=150, caption="SAFFIRE")

    # üì¶ Section de chargement des donn√©es
    st.markdown("## Chargement des Donn√©es")
    train_data = st.file_uploader("Importer les donn√©es d'entra√Ænement (ZIP)", type=["zip"])
    train_dir = "temp_train_dir"

    def extract_zip(zip_file, extract_to):
        """
        üóÇÔ∏è Fonction d'extraction intelligente du ZIP
        
        Analogie : Comme d√©baller un colis avec plusieurs bo√Ætes imbriqu√©es.
        Cette fonction est assez intelligente pour comprendre si votre ZIP a
        une bo√Æte suppl√©mentaire √† l'int√©rieur et l'enl√®ve automatiquement.
        
        Args:
            zip_file: Le fichier ZIP √† d√©baller
            extract_to: O√π mettre le contenu d√©ball√©
        """
        # üßπ Nettoyage pr√©alable - Comme vider une bo√Æte avant de la remplir
        if os.path.exists(extract_to):
            shutil.rmtree(extract_to)
        os.makedirs(extract_to)
        
        with zipfile.ZipFile(zip_file, 'r') as zip_ref:
            # üìÇ Extraction dans un dossier temporaire - Zone de tri
            temp_extract = extract_to + "_temp"
            zip_ref.extractall(temp_extract)
            
            # üîç Analyse de la structure - Comme inspecter le contenu d'un colis
            items = os.listdir(temp_extract)
            
            # Cas 1: Structure directe (parfaite) - Comme un colis bien organis√©
            if all(os.path.isdir(os.path.join(temp_extract, item)) for item in items if not item.startswith('.')):
                for item in items:
                    if not item.startswith('.') and not item == '__MACOSX':
                        source_path = os.path.join(temp_extract, item)
                        dest_path = os.path.join(extract_to, item)
                        if os.path.isdir(source_path):
                            shutil.copytree(source_path, dest_path)
                        else:
                            shutil.copy2(source_path, dest_path)
            
            # Cas 2: Dossier parent en trop - Comme une bo√Æte dans une bo√Æte
            else:
                parent_folder = None
                for item in items:
                    item_path = os.path.join(temp_extract, item)
                    if os.path.isdir(item_path) and not item.startswith('.') and item != '__MACOSX':
                        sub_items = os.listdir(item_path)
                        if any(os.path.isdir(os.path.join(item_path, sub)) for sub in sub_items if not sub.startswith('.')):
                            parent_folder = item_path
                            break
                
                if parent_folder:
                    # Extraction du contenu de la bo√Æte interne
                    for item in os.listdir(parent_folder):
                        if not item.startswith('.') and not item == '__MACOSX':
                            source_path = os.path.join(parent_folder, item)
                            dest_path = os.path.join(extract_to, item)
                            if os.path.isdir(source_path):
                                shutil.copytree(source_path, dest_path)
                            else:
                                shutil.copy2(source_path, dest_path)
                else:
                    # Structure non reconnue - Copie tout tel quel
                    for item in items:
                        if not item.startswith('.') and not item == '__MACOSX':
                            source_path = os.path.join(temp_extract, item)
                            dest_path = os.path.join(extract_to, item)
                            if os.path.isdir(source_path):
                                shutil.copytree(source_path, dest_path)
                            else:
                                shutil.copy2(source_path, dest_path)
            
            # üßπ Nettoyage du dossier temporaire - Ranger apr√®s le tri
            shutil.rmtree(temp_extract)
            
            # üìä Rapport de ce qui a √©t√© trouv√© - Inventaire
            classes_found = [d for d in os.listdir(extract_to) if os.path.isdir(os.path.join(extract_to, d))]
            st.info(f"üìÅ Structure d√©tect√©e : {len(classes_found)} classes trouv√©es")
            st.write("Classes d√©tect√©es :", ", ".join(classes_found))

    # ‚öôÔ∏è Configuration des hyperparam√®tres selon le mode
    if mode == "Manual":
        st.sidebar.markdown("### üéõÔ∏è Hyperparam√®tres d'Entra√Ænement")
        
        # üß† Optimiseur - Le "professeur" qui guide l'apprentissage
        optimizer_choice = st.sidebar.selectbox(
            "Optimiseur (le 'professeur' de votre IA):", 
            ("Adam", "SGD", "RMSprop"),
            help="Adam = professeur patient et intelligent | SGD = professeur simple mais efficace | RMSprop = professeur qui s'adapte"
        )
        
        # üìè Taux d'apprentissage - La "vitesse d'apprentissage"
        learning_rate = st.sidebar.number_input(
            "Taux d'apprentissage (vitesse d'apprentissage):", 
            min_value=0.0001, max_value=1.0, value=0.001, step=0.0001, format="%f",
            help="Trop rapide = votre IA devient confuse | Trop lent = votre IA apprend tr√®s lentement"
        )
        
        # üîÑ Epochs - Nombre de "cours" complets
        epochs = st.sidebar.number_input(
            "Nombre d'epochs (cours complets):", 
            min_value=1, max_value=100, value=30, step=1,
            help="Comme le nombre de fois que votre IA revoit tout le manuel d'apprentissage"
        )
        
        # üì¶ Taille des batchs - Taille des "groupes d'√©tude"
        batch_size = st.sidebar.number_input(
            "Taille des batchs (taille des groupes d'√©tude):", 
            min_value=1, max_value=128, value=32, step=1,
            help="Nombre d'images que votre IA √©tudie en m√™me temps. Plus grand = plus rapide mais plus de m√©moire"
        )
        
        # üèóÔ∏è Architecture du r√©seau neuronal
        st.sidebar.markdown("### üèóÔ∏è Architecture du R√©seau")
        
        # Nombre de couches convolutives - "√âtages de d√©tection"
        num_conv_layers = st.sidebar.slider(
            "Nombre de couches convolutives (√©tages de d√©tection):", 
            min_value=1, max_value=5, value=3,
            help="Chaque √©tage d√©tecte des patterns plus complexes : 1=lignes, 2=formes, 3=objets"
        )
        
        # Filtres par couche - "Nombre de d√©tecteurs par √©tage"
        filters_per_layer = []
        for i in range(num_conv_layers):
            filters = st.sidebar.number_input(
                f"Filtres couche {i+1} (d√©tecteurs √† l'√©tage {i+1}):", 
                min_value=8, max_value=512, value=16 * (2**i), step=8,
                help=f"√âtage {i+1}: Plus de d√©tecteurs = plus de pr√©cision mais plus lent"
            )
            filters_per_layer.append(filters)
        
        # Neurones dans la couche dense - "Taille du cerveau de d√©cision"
        dense_units = st.sidebar.number_input(
            "Neurones Dense (taille du cerveau de d√©cision):", 
            min_value=8, max_value=512, value=64, step=8,
            help="Le cerveau final qui prend la d√©cision. Plus gros = plus intelligent mais plus lent"
        )
        
        # Taux de Dropout - "Pourcentage d'oubli volontaire"
        dropout_rate = st.sidebar.slider(
            "Taux de Dropout (oubli volontaire pour √©viter le par-c≈ìur):", 
            min_value=0.0, max_value=0.9, value=0.5, step=0.05,
            help="Comme dire √† votre IA d'oublier 50% de ce qu'elle voit pour ne pas apprendre par c≈ìur"
        )
        
        # Fonction d'activation - "Type de r√©flexion"
        activation_function = st.sidebar.selectbox(
            "Fonction d'activation (type de r√©flexion):", 
            ("relu", "sigmoid", "tanh"),
            help="ReLU = pens√©e simple et rapide | Sigmoid = pens√©e nuanc√©e | Tanh = pens√©e √©quilibr√©e"
        )
        
        # üõ°Ô∏è R√©gularisation avanc√©e
        st.sidebar.markdown("### üõ°Ô∏è R√©gularisation Avanc√©e")
        
        # R√©gularisation L2 - "P√©nalit√© pour la complexit√©"
        l2_regularization = st.sidebar.slider(
            "R√©gularisation L2 (p√©nalit√© complexit√©):", 
            min_value=0.0, max_value=0.01, value=0.0001, step=0.0001, format="%.4f",
            help="Emp√™che votre IA de devenir trop compliqu√©e. Comme limiter le nombre de r√®gles qu'elle peut apprendre"
        )
        
        # Patience pour l'arr√™t pr√©coce
        early_stopping_patience = st.sidebar.number_input(
            "Patience arr√™t pr√©coce (patience avant abandon):", 
            min_value=5, max_value=50, value=10, step=1,
            help="Nombre d'epochs sans am√©lioration avant d'arr√™ter l'entra√Ænement automatiquement"
        )
        
        # R√©duction du learning rate
        reduce_lr_patience = st.sidebar.number_input(
            "Patience r√©duction LR (patience avant ralentissement):", 
            min_value=3, max_value=20, value=5, step=1,
            help="Si pas d'am√©lioration, ralentir l'apprentissage au lieu d'abandonner"
        )
        
        reduce_lr_factor = st.sidebar.slider(
            "Facteur r√©duction LR (niveau de ralentissement):", 
            min_value=0.1, max_value=0.9, value=0.5, step=0.1,
            help="De combien ralentir l'apprentissage (0.5 = moiti√© moins vite)"
        )
        
    else:
        # ü§ñ Mode automatique - Configuration optimis√©e par d√©faut
        st.sidebar.markdown("### ü§ñ Configuration Automatique Optimis√©e")
        st.sidebar.write("‚úÖ Param√®tres optimaux s√©lectionn√©s automatiquement")
        
        # Valeurs par d√©faut optimis√©es
        optimizer_choice = "Adam"
        learning_rate = 0.001
        epochs = 20
        batch_size = 32
        num_conv_layers = 3
        filters_per_layer = [32, 64, 128]
        dense_units = 128
        dropout_rate = 0.3
        activation_function = "relu"
        l2_regularization = 0.0001
        early_stopping_patience = 10
        reduce_lr_patience = 5
        reduce_lr_factor = 0.5

    # üöÄ Bouton d'entra√Ænement
    if st.button("üöÄ D√©marrer l'entra√Ænement", type="primary"):
        if train_data is not None:
            # üì¶ Extraction et pr√©paration des donn√©es
            extract_zip(train_data, train_dir)
            
            # üé≠ Augmentation des donn√©es - "Cr√©er des variations"
            datagen = ImageDataGenerator(
                rescale=1./255,  # Normalisation - ramener les valeurs entre 0 et 1
                validation_split=0.2,  # 20% pour tester, 80% pour apprendre
                # Options d'augmentation possibles (comment√©es pour le mode de base)
                # rotation_range=20,  # Rotation al√©atoire
                # width_shift_range=0.2,  # D√©calage horizontal
                # height_shift_range=0.2,  # D√©calage vertical
                # horizontal_flip=True  # Miroir horizontal
            )
            
            # üè≠ Cr√©ation des g√©n√©rateurs de donn√©es - "Cha√Ænes de production"
            train_generator = datagen.flow_from_directory(
                train_dir, 
                target_size=(128, 128),  # Redimensionner toutes les images
                batch_size=batch_size, 
                class_mode='categorical',  # Classification multi-classes
                subset='training'
            )
            
            val_generator = datagen.flow_from_directory(
                train_dir, 
                target_size=(128, 128), 
                batch_size=batch_size, 
                class_mode='categorical', 
                subset='validation'
            )
            
            # üìä Analyse des donn√©es
            num_classes = len(train_generator.class_indices)
            class_names = list(train_generator.class_indices.keys())
            st.info(f"üéØ {num_classes} classes d√©tect√©es : {', '.join(class_names)}")

            # üß† S√©lection de l'optimiseur - "Choix du professeur"
            if optimizer_choice == "Adam":
                # Adam = Professeur intelligent qui s'adapte
                optimizer = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999)
            elif optimizer_choice == "SGD":
                # SGD = Professeur traditionnel mais efficace
                optimizer = SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)
            elif optimizer_choice == "RMSprop":
                # RMSprop = Professeur qui s'adapte aux difficult√©s
                optimizer = RMSprop(learning_rate=learning_rate, rho=0.9)

            # üèóÔ∏è Construction du mod√®le - "Assemblage du cerveau"
            model = Sequential()
            
            # üëÅÔ∏è Couches convolutives - "√âtages de d√©tection visuelle"
            for i, filters in enumerate(filters_per_layer):
                model.add(Conv2D(
                    filters, (3, 3),  # Filtres 3x3 comme des petites loupes
                    activation=activation_function,
                    kernel_regularizer=l2(l2_regularization),  # √âviter la sur-sp√©cialisation
                    padding='same',  # Garder la m√™me taille d'image
                    name=f'conv2d_{i+1}'
                ))
                
                # üßº Normalisation par batch - "Standardisation"
                model.add(BatchNormalization(name=f'batch_norm_{i+1}'))
                
                # üîΩ Max Pooling - "R√©sum√© de zone"
                model.add(MaxPooling2D(pool_size=(2, 2), name=f'max_pool_{i+1}'))
            
            # üåç Globalisation des informations
            model.add(GlobalAveragePooling2D(name='global_avg_pool'))
            
            # üß† Couche de d√©cision dense
            model.add(Dense(
                dense_units, 
                activation=activation_function,
                kernel_regularizer=l2(l2_regularization),
                name='dense_decision'
            ))
            
            # üé≤ Dropout - "Oubli volontaire pour √©viter le par-c≈ìur"
            model.add(Dropout(dropout_rate, name='dropout'))
            
            # üéØ Couche de sortie finale
            model.add(Dense(
                num_classes, 
                activation='softmax',  # Probabilit√©s qui totalisent 100%
                name='output'
            ))

            # ‚öôÔ∏è Compilation du mod√®le - "Configuration finale"
            model.compile(
                optimizer=optimizer,
                loss='categorical_crossentropy',  # Fonction de co√ªt pour classification
                metrics=['accuracy']  # Mesurer la pr√©cision
            )
            
            # üìä Affichage de l'architecture
            st.write("üèóÔ∏è **Architecture du mod√®le cr√©√© :**")
            model_summary = []
            model.summary(print_fn=lambda x: model_summary.append(x))
            st.text('\n'.join(model_summary))

            # üéì Entra√Ænement avec callbacks - "Supervision intelligente"
            from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
            
            callbacks = [
                # üõë Arr√™t pr√©coce si pas d'am√©lioration
                EarlyStopping(
                    monitor='val_accuracy',
                    patience=early_stopping_patience,
                    restore_best_weights=True,
                    verbose=1
                ),
                # üìâ R√©duction du learning rate si plateau
                ReduceLROnPlateau(
                    monitor='val_accuracy',
                    factor=reduce_lr_factor,
                    patience=reduce_lr_patience,
                    min_lr=1e-7,
                    verbose=1
                )
            ]
            
            # üöÄ Lancement de l'entra√Ænement
            with st.spinner('üéì Entra√Ænement en cours... Votre IA apprend !'):
                history = model.fit(
                    train_generator,
                    epochs=epochs,
                    validation_data=val_generator,
                    callbacks=callbacks,
                    verbose=1
                )
            
            # üíæ Sauvegarde du mod√®le entra√Æn√©
            model.save(model_path)
            st.success("‚úÖ Entra√Ænement termin√© et mod√®le sauvegard√© !")

            # üìà Affichage des courbes de convergence - "Graphiques de progression"
            st.markdown("## üìà Courbes de Convergence")
            fig, ax = plt.subplots(1, 2, figsize=(14, 5))
            
            # Graphique de la perte
            ax[0].plot(history.history['loss'], label='Perte Entra√Ænement', color='blue')
            ax[0].plot(history.history['val_loss'], label='Perte Validation', color='red')
            ax[0].set_title('√âvolution de la Perte (plus bas = mieux)')
            ax[0].set_xlabel('Epochs')
            ax[0].set_ylabel('Perte')
            ax[0].legend()
            ax[0].grid(True, alpha=0.3)

            # Graphique de la pr√©cision
            ax[1].plot(history.history['accuracy'], label='Pr√©cision Entra√Ænement', color='green')
            ax[1].plot(history.history['val_accuracy'], label='Pr√©cision Validation', color='orange')
            ax[1].set_title('√âvolution de la Pr√©cision (plus haut = mieux)')
            ax[1].set_xlabel('Epochs')
            ax[1].set_ylabel('Pr√©cision')
            ax[1].legend()
            ax[1].grid(True, alpha=0.3)
            
            st.pyplot(fig)

            # üéØ Matrice de confusion - "Tableau des erreurs"
            st.markdown("## üéØ Matrice de Confusion")
            if val_generator.samples > 0:
                # Pr√©dictions sur les donn√©es de validation
                val_preds = model.predict(val_generator, verbose=0)
                y_pred = np.argmax(val_preds, axis=1)
                y_true = val_generator.classes

                # Mise √† jour des noms de classes
                if not class_names:
                    class_names = list(val_generator.class_indices.keys())

                # Cr√©ation et affichage de la matrice
                cm = confusion_matrix(y_true, y_pred)
                fig_cm, ax_cm = plt.subplots(figsize=(8, 6))
                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
                disp.plot(ax=ax_cm, cmap='Blues', colorbar=True)
                ax_cm.set_title("Matrice de Confusion\n(Diagonale = bonnes pr√©dictions)")
                st.pyplot(fig_cm)
                
                # Statistiques d√©taill√©es
                accuracy = np.trace(cm) / np.sum(cm)
                st.write(f"**Pr√©cision globale :** {accuracy:.2%}")
                
            else:
                st.warning("‚ö†Ô∏è Aucune donn√©e de validation disponible.")

    # üîÆ Section de pr√©diction
    st.markdown("## üîÆ Pr√©diction sur une Image")
    image_file = st.file_uploader("Choisissez une image pour pr√©diction", type=["jpg", "png", "jpeg"])
    
    if image_file and st.button("üîÆ Pr√©dire", type="secondary"):
        # Chargement du mod√®le si n√©cessaire
        if model is None and os.path.exists(model_path):
            model = load_model(model_path)
            
        if model is not None:
            # Pr√©processing de l'image
            img = load_img(image_file, target_size=(128, 128))
            img_array = img_to_array(img)
            img_array = np.expand_dims(img_array, axis=0) / 255.0  # Normalisation
            
            # Pr√©diction
            prediction = model.predict(img_array)[0]
            predicted_class = np.argmax(prediction)
            predicted_label = class_names[predicted_class] if class_names else f"Classe {predicted_class}"
            confidence = round(prediction[predicted_class] * 100, 2)
            
            # Affichage des r√©sultats
            col1, col2 = st.columns(2)
            with col1:
                st.image(image_file, caption="üñºÔ∏è Image √† analyser")
            with col2:
                st.markdown("### üéØ R√©sultat de la Pr√©diction")
                st.success(f"**Classe pr√©dite :** {predicted_label}")
                st.info(f"**Confiance :** {confidence}%")
                
                # Barre de progression pour la confiance
                st.progress(confidence / 100)
                
                # Top 3 des pr√©dictions
                if len(prediction) > 1:
                    st.markdown("#### üèÜ Top 3 des pr√©dictions")
                    top_3_indices = np.argsort(prediction)[-3:][::-1]
                    for i, idx in enumerate(top_3_indices):
                        class_name = class_names[idx] if class_names else f"Classe {idx}"
                        score = prediction[idx] * 100
                        st.write(f"{i+1}. {class_name}: {score:.1f}%")
        else:
            st.error("‚ùó Aucun mod√®le charg√©. Veuillez d'abord entra√Æner un mod√®le.")

# üé® MODULE DE TRANSFERT DE STYLE
elif main_mode == "Transfert de Style":
    st.markdown("## üé® Module de Transfert de Style Neural")
    st.write("*Transformez vos photos en ≈ìuvres d'art en combinant le contenu d'une image avec le style d'une autre*")
    
    # üéõÔ∏è Hyperparam√®tres principaux dans la sidebar
    st.sidebar.markdown("### üé® Param√®tres Artistiques")
    
    # Poids du style - "Intensit√© artistique"
    style_weight = st.sidebar.slider(
        "Poids du style (intensit√© artistique):", 
        1e-2, 1e6, 1e4, step=1e3, format="%.0e",
        help="Plus √©lev√© = plus artistique mais moins ressemblant √† l'original"
    )
    
    # Poids du contenu - "Pr√©servation de l'original"
    content_weight = st.sidebar.slider(
        "Poids du contenu (pr√©servation original):", 
        1e0, 1e4, 1e3, step=1e2, format="%.0e",
        help="Plus √©lev√© = plus fid√®le √† l'image originale"
    )
    
    # Nombre d'it√©rations - "Temps de cr√©ation"
    iterations = st.sidebar.number_input(
        "Nombre d'it√©rations (temps de cr√©ation):", 
        min_value=10, max_value=1000, value=100, step=10,
        help="Plus d'it√©rations = meilleure qualit√© mais plus lent"
    )
    
    # üîß Param√®tres avanc√©s
    st.sidebar.markdown("### üîß Param√®tres Avanc√©s")
    
    # Taux d'apprentissage - "Vitesse d'am√©lioration"
    learning_rate = st.sidebar.slider(
        "Taux d'apprentissage (vitesse am√©lioration):", 
        0.001, 0.1, 0.01, step=0.001,
        help="Plus rapide = convergence rapide mais risque d'instabilit√©"
    )
    
    # Taille maximale d'image - "Qualit√© vs vitesse"
    max_image_size = st.sidebar.selectbox(
        "Taille max image (qualit√© vs vitesse):",
        [256, 384, 512, 768, 1024],
        index=2,  # 512 par d√©faut
        help="Plus grand = meilleure qualit√© mais beaucoup plus lent"
    )
    
    # Param√®tres de l'optimiseur Adam
    st.sidebar.markdown("#### ‚öôÔ∏è Optimiseur Adam")
    
    beta1 = st.sidebar.slider(
        "Beta1 (m√©moire gradient):",
        0.8, 0.999, 0.99, step=0.01,
        help="M√©moire des gradients pr√©c√©dents. Plus √©lev√© = plus de m√©moire"
    )
    
    beta2 = st.sidebar.slider(
        "Beta2 (m√©moire variance):",
        0.9, 0.999, 0.999, step=0.001,
        help="M√©moire de la variance. G√©n√©ralement proche de 1.0"
    )
    
    epsilon = st.sidebar.selectbox(
        "Epsilon (stabilit√© num√©rique):",
        [1e-8, 1e-4, 1e-1, 1e-0],
        index=2,  # 1e-1 par d√©faut
        format_func=lambda x: f"{x:.0e}",
        help="√âvite la division par z√©ro. Plus √©lev√© = plus stable"
    )
    
    # üéöÔ∏è Contr√¥le des couches de style
    st.sidebar.markdown("#### üé® Couches de Style Actives")
    st.sidebar.write("*Chaque couche capture diff√©rents aspects du style*")
    
    use_block1 = st.sidebar.checkbox(
        "Block1 (textures fines)", 
        value=True,
        help="Capture les d√©tails fins : lignes, points, textures de base"
    )
    use_block2 = st.sidebar.checkbox(
        "Block2 (motifs simples)", 
        value=True,
        help="Capture les motifs simples : rayures, cercles, formes g√©om√©triques"
    )
    use_block3 = st.sidebar.checkbox(
        "Block3 (structures moyennes)", 
        value=True,
        help="Capture les structures moyennes : objets partiels, compositions"
    )
    use_block4 = st.sidebar.checkbox(
        "Block4 (formes complexes)", 
        value=True,
        help="Capture les formes complexes : objets entiers, relations spatiales"
    )
    use_block5 = st.sidebar.checkbox(
        "Block5 (composition globale)", 
        value=True,
        help="Capture la composition globale : distribution des √©l√©ments, style g√©n√©ral"
    )
    
    # üé≠ Options de pr√©processing
    st.sidebar.markdown("#### üé≠ Pr√©processing des Images")
    
    preserve_colors = st.sidebar.checkbox(
        "Pr√©server couleurs originales",
        value=False,
        help="Garde les couleurs de l'image de contenu et applique seulement la texture du style"
    )
    
    enhance_contrast = st.sidebar.slider(
        "Am√©lioration contraste:",
        0.5, 2.0, 1.0, step=0.1,
        help="Ajuste le contraste de l'image finale. 1.0 = normal"
    )
    
    color_saturation = st.sidebar.slider(
        "Saturation couleurs:",
        0.0, 2.0, 1.0, step=0.1,
        help="Ajuste la vivacit√© des couleurs. 1.0 = normal"
    )
    
    # üîç Options de d√©bogage
    st.sidebar.markdown("### üîç D√©bogage et Analyse")
    
    debug_mode = st.sidebar.checkbox(
        "Mode d√©bogage", 
        value=True,
        help="Affiche les images pr√©process√©es et aper√ßus pendant l'entra√Ænement"
    )
    
    show_diagnostics = st.sidebar.checkbox(
        "Diagnostic d√©taill√©", 
        value=False,
        help="Analyse technique des images upload√©es (format, taille, valeurs)"
    )
    
    show_loss_breakdown = st.sidebar.checkbox(
        "D√©tail des pertes",
        value=False, 
        help="Affiche s√©par√©ment la perte de style et de contenu"
    )
    
    preview_frequency = st.sidebar.number_input(
        "Fr√©quence aper√ßus:",
        min_value=5, max_value=50, value=20, step=5,
        help="Montrer un aper√ßu toutes les X it√©rations"
    )

    def diagnose_image(image_file, name):
        """
        üî¨ Fonction de diagnostic d'image
        
        Analogie : Comme un m√©decin qui examine un patient.
        Cette fonction regarde tous les "signes vitaux" de votre image :
        - Sa taille, son format, ses couleurs
        - D√©tecte s'il y a des probl√®mes potentiels
        
        Args:
            image_file: L'image √† diagnostiquer
            name: Nom de l'image pour l'affichage
        """
        img = Image.open(image_file)
        img_array = np.array(img)
        
        st.write(f"**üìä Diagnostic de l'image {name}:**")
        st.write(f"- Format original: {img.format} {'‚úÖ' if img.format in ['JPEG', 'PNG'] else '‚ö†Ô∏è'}")
        st.write(f"- Mode couleur: {img.mode} {'‚úÖ' if img.mode == 'RGB' else '‚ö†Ô∏è'}")
        st.write(f"- Dimensions: {img.size[0]}√ó{img.size[1]} pixels")
        st.write(f"- Forme du array: {img_array.shape}")
        st.write(f"- Type de donn√©es: {img_array.dtype}")
        st.write(f"- Plage valeurs: {img_array.min()} ‚Üí {img_array.max()}")
        
        # Analyse des couleurs par canal
        if len(img_array.shape) == 3:
            r_mean = img_array[:,:,0].mean()
            g_mean = img_array[:,:,1].mean()
            b_mean = img_array[:,:,2].mean()
            st.write(f"- Moyennes RGB: R={r_mean:.1f}, G={g_mean:.1f}, B={b_mean:.1f}")
            
            # D√©tection de probl√®mes potentiels
            if abs(r_mean - g_mean) < 5 and abs(g_mean - b_mean) < 5:
                st.warning("‚ö†Ô∏è Image semble en niveaux de gris")
            if img_array.max() <= 1:
                st.info("‚ÑπÔ∏è Image d√©j√† normalis√©e [0,1]")
        
        return img_array

    # üé® Fonctions pour le transfert de style
    @st.cache_resource
    def load_vgg_model():
        """
        üß† Chargement du mod√®le VGG19 pr√©-entra√Æn√©
        
        Analogie : Comme emprunter les yeux d'un expert en art.
        VGG19 a √©t√© entra√Æn√© sur des millions d'images et "sait" reconna√Ætre
        les formes, textures et styles artistiques.
        
        Returns:
            Mod√®le VGG19 fig√© (non-entra√Ænable)
        """
        vgg = VGG19(include_top=False, weights='imagenet')
        vgg.trainable = False  # On fige le mod√®le - pas d'apprentissage
        return vgg

    def preprocess_image(image_path, max_dim=512):
        """
        üõ†Ô∏è Pr√©processing intelligent des images
        
        Analogie : Comme pr√©parer une toile avant de peindre.
        Cette fonction nettoie, redimensionne et normalise l'image
        pour qu'elle soit parfaite pour le transfert de style.
        
        Args:
            image_path: Chemin vers l'image ou objet file Streamlit
            max_dim: Dimension maximale (plus grand = plus lent)
            
        Returns:
            Tensor TensorFlow normalis√© et redimensionn√©
        """
        if isinstance(image_path, str):
            # Chargement depuis fichier local
            img = tf.io.read_file(image_path)
            img = tf.image.decode_image(img, channels=3)
        else:
            # Chargement depuis Streamlit file uploader
            img = Image.open(image_path)
            img = np.array(img)
            
            # üé® Gestion des diff√©rents formats d'image
            if len(img.shape) == 2:  # Image en niveaux de gris
                # Conversion en RGB en dupliquant le canal
                img = np.stack([img] * 3, axis=-1)
                st.info("‚ÑπÔ∏è Image convertie de niveaux de gris vers RGB")
                
            elif len(img.shape) == 3 and img.shape[-1] == 4:  # RGBA (avec transparence)
                # Suppression du canal alpha (transparence)
                img = img[:, :, :3]
                st.info("‚ÑπÔ∏è Canal alpha supprim√© (RGBA ‚Üí RGB)")
            
            # üìä Normalisation intelligente des valeurs
            if img.dtype == np.uint8:
                # Conversion standard uint8 [0,255] ‚Üí float32 [0,1]
                img = img.astype(np.float32) / 255.0
            elif img.dtype == np.float32 and img.max() > 1.0:
                # Image float32 mal normalis√©e
                img = img / 255.0
                st.info("‚ÑπÔ∏è Image float32 renormalis√©e")
            
            # Conversion en tensor TensorFlow
            img = tf.constant(img, dtype=tf.float32)
        
        # üîÑ V√©rifications et conversions finales
        if img.dtype != tf.float32:
            img = tf.image.convert_image_dtype(img, tf.float32)
        
        # Assurer que l'image a exactement 3 canaux
        if len(img.shape) == 3 and img.shape[-1] != 3:
            if img.shape[-1] == 1:  # Niveaux de gris
                img = tf.image.grayscale_to_rgb(img)
            elif img.shape[-1] == 4:  # RGBA
                img = img[:, :, :3]
        
        # üìè Redimensionnement proportionnel
        # Analogie : Ajuster la taille d'une photo sans la d√©former
        shape = tf.cast(tf.shape(img)[:-1], tf.float32)
        long_dim = max(shape)
        scale = max_dim / long_dim
        
        new_shape = tf.cast(shape * scale, tf.int32)
        img = tf.image.resize(img, new_shape)
        
        # üì¶ Ajout de la dimension batch (pour le traitement par lots)
        img = img[tf.newaxis, :]
        
        # üõ°Ô∏è S√©curit√© : s'assurer que les valeurs sont dans [0,1]
        img = tf.clip_by_value(img, 0.0, 1.0)
        
        return img

    def deprocess_image(processed_img):
        """
        üñºÔ∏è Conversion de l'image trait√©e vers format affichable
        
        Analogie : Comme d√©velopper une photo depuis un n√©gatif.
        Convertit le tensor TensorFlow normalis√© en image PNG/JPEG standard.
        
        Args:
            processed_img: Tensor TensorFlow [0,1]
            
        Returns:
            Array NumPy uint8 [0,255] pr√™t pour l'affichage
        """
        x = processed_img.copy()
        
        # Suppression de la dimension batch si pr√©sente
        if len(x.shape) == 4:
            x = np.squeeze(x, 0)
        
        # üõ°Ô∏è S√©curit√© : clipper les valeurs dans [0,1]
        x = np.clip(x, 0, 1)
        
        # üé® Conversion vers format d'affichage [0,255]
        x = (x * 255).astype('uint8')
        
        return x

    def apply_color_adjustments(img, enhance_contrast, color_saturation):
        """
        üé® Application d'ajustements colorim√©triques
        
        Analogie : Comme ajuster les r√©glages d'un t√©l√©viseur.
        Modifie le contraste et la saturation pour am√©liorer le rendu final.
        
        Args:
            img: Image √† ajuster
            enhance_contrast: Facteur de contraste (1.0 = normal)
            color_saturation: Facteur de saturation (1.0 = normal)
            
        Returns:
            Image ajust√©e
        """
        # Ajustement du contraste
        if enhance_contrast != 1.0:
            img = tf.image.adjust_contrast(img, enhance_contrast)
        
        # Ajustement de la saturation
        if color_saturation != 1.0:
            img = tf.image.adjust_saturation(img, color_saturation)
        
        # Re-clipper apr√®s ajustements
        img = tf.clip_by_value(img, 0.0, 1.0)
        
        return img

    def gram_matrix(input_tensor):
        """
        üî¢ Calcul de la matrice de Gram pour capturer le style
        
        Analogie : Comme analyser les "empreintes digitales" artistiques.
        La matrice de Gram capture les corr√©lations entre diff√©rentes 
        caract√©ristiques visuelles, cr√©ant une signature unique du style.
        
        Math : G[i,j] = Œ£(F[k,i] √ó F[k,j]) / N
        O√π F sont les features et N le nombre de positions
        
        Args:
            input_tensor: Features extraites par VGG19
            
        Returns:
            Matrice de Gram (corr√©lations de style)
        """
        # üìè V√©rification et ajustement des dimensions
        if len(input_tensor.shape) == 3:
            input_tensor = tf.expand_dims(input_tensor, 0)
        
        # üìä Extraction des dimensions
        batch_size = tf.shape(input_tensor)[0]
        height = tf.shape(input_tensor)[1] 
        width = tf.shape(input_tensor)[2]
        channels = tf.shape(input_tensor)[3]
        
        # üîÑ Reshape en matrice 2D : (positions, features)
        # Analogie : Comme √©taler toutes les "observations" en lignes
        features = tf.reshape(input_tensor, (batch_size, height * width, channels))
        
        # üßÆ Calcul de la matrice de Gram : F^T √ó F
        # Analogie : Calculer toutes les corr√©lations entre features
        gram = tf.matmul(features, features, transpose_a=True)
        
        # ‚ûó Normalisation par le nombre de positions
        # Analogie : Faire une moyenne pour que la taille d'image n'influe pas
        num_locations = tf.cast(height * width, tf.float32)
        
        return gram / num_locations

    def build_style_layers_list():
        """
        üèóÔ∏è Construction de la liste des couches de style actives
        
        Retourne la liste des couches VGG19 √† utiliser selon les checkboxes
        """
        style_layers = []
        if use_block1:
            style_layers.append('block1_conv1')
        if use_block2:
            style_layers.append('block2_conv1') 
        if use_block3:
            style_layers.append('block3_conv1')
        if use_block4:
            style_layers.append('block4_conv1')
        if use_block5:
            style_layers.append('block5_conv1')
            
        return style_layers

    def style_content_loss(outputs, style_targets, content_targets, style_weight, content_weight):
        """
        ‚öñÔ∏è Calcul de la perte combin√©e style + contenu
        
        Analogie : Comme noter un devoir avec deux crit√®res :
        - Respect du style artistique (originalit√©)
        - Pr√©servation du contenu (fid√©lit√©)
        
        Args:
            outputs: Sorties actuelles du mod√®le
            style_targets: Cibles de style (ce qu'on veut atteindre)
            content_targets: Cibles de contenu (ce qu'on veut pr√©server)
            style_weight: Importance du style
            content_weight: Importance du contenu
            
        Returns:
            Perte totale √† minimiser
        """
        style_outputs = outputs['style']
        content_outputs = outputs['content']
        
        # üé® Calcul de la perte de style
        # Analogie : Mesurer √† quel point le style diff√®re de l'≈ìuvre de r√©f√©rence
        style_loss = 0
        for name in style_targets.keys():
            # Diff√©rence quadratique entre matrices de Gram
            layer_loss = tf.reduce_mean((style_outputs[name] - style_targets[name])**2)
            style_loss += layer_loss
            
            # Debug : afficher les pertes par couche si demand√©
            if show_loss_breakdown:
                st.write(f"üé® Perte style {name}: {float(layer_loss):.4f}")
        
        # Normalisation par le nombre de couches de style
        style_loss *= style_weight / len(style_targets)
        
        # üì∑ Calcul de la perte de contenu  
        # Analogie : Mesurer √† quel point on s'√©loigne de l'image originale
        content_loss = 0
        for name in content_targets.keys():
            # Diff√©rence quadratique entre features de contenu
            layer_loss = tf.reduce_mean((content_outputs[name] - content_targets[name])**2)
            content_loss += layer_loss
            
            if show_loss_breakdown:
                st.write(f"üì∑ Perte contenu {name}: {float(layer_loss):.4f}")
        
        # Normalisation par le nombre de couches de contenu
        content_loss *= content_weight / len(content_targets)
        
        # ‚öñÔ∏è Perte totale = Style + Contenu
        total_loss = style_loss + content_loss
        
        # Debug d√©taill√©
        if show_loss_breakdown:
            st.write(f"**Total - Style: {float(style_loss):.4f}, Contenu: {float(content_loss):.4f}**")
        
        return total_loss

    class StyleContentModel(tf.keras.models.Model):
        """
        üé≠ Mod√®le d'extraction de style et contenu
        
        Analogie : Comme un critique d'art expert qui peut analyser
        s√©par√©ment le style artistique et le contenu d'une ≈ìuvre.
        
        Cette classe utilise VGG19 pr√©-entra√Æn√© pour extraire :
        - Les caract√©ristiques de style (matrices de Gram)
        - Les caract√©ristiques de contenu (features s√©mantiques)
        """
        
        def __init__(self, style_layers, content_layers):
            """
            üèóÔ∏è Initialisation du mod√®le extracteur
            
            Args:
                style_layers: Liste des couches VGG19 pour le style
                content_layers: Liste des couches VGG19 pour le contenu
            """
            super(StyleContentModel, self).__init__()
            
            # üß† Chargement du mod√®le VGG19 pr√©-entra√Æn√©
            self.vgg = load_vgg_model()
            self.style_layers = style_layers
            self.content_layers = content_layers
            self.num_style_layers = len(style_layers)
            self.vgg.trainable = False  # Mod√®le fig√©
            
            # üîå Construction d'un extracteur unifi√© pour efficacit√©
            # Analogie : Cr√©er un seul passage au lieu de plusieurs allers-retours
            style_outputs = [self.vgg.get_layer(name).output for name in style_layers]
            content_outputs = [self.vgg.get_layer(name).output for name in content_layers]
            model_outputs = style_outputs + content_outputs
            
            # üè≠ Mod√®le d'extraction unifi√©
            self.feature_extractor = tf.keras.Model([self.vgg.input], model_outputs)

        def call(self, inputs):
            """
            üîÑ Forward pass - Extraction des caract√©ristiques
            
            Analogie : Passer une image dans les "yeux" de l'expert
            pour qu'il analyse le style et le contenu.
            
            Args:
                inputs: Image d'entr√©e normalis√©e [0,1]
                
            Returns:
                Dictionnaire avec features de style et contenu
            """
            # üìè V√©rification des dimensions d'entr√©e
            if len(inputs.shape) == 3:
                inputs = tf.expand_dims(inputs, 0)
            
            # üõ°Ô∏è S√©curit√© : clipper les valeurs dans [0,1]
            inputs = tf.clip_by_value(inputs, 0.0, 1.0)
            
            # üé® Ajustements colorim√©triques si demand√©s
            inputs = apply_color_adjustments(inputs, enhance_contrast, color_saturation)
            
            # üîÑ Pr√©processing pour VGG19
            # Conversion [0,1] ‚Üí [0,255] puis normalisation ImageNet
            inputs_scaled = inputs * 255.0
            preprocessed_input = preprocess_input(inputs_scaled)
            
            # üè≠ Extraction des features via le mod√®le unifi√©
            outputs = self.feature_extractor(preprocessed_input)
            
            # üìä S√©paration des outputs style et contenu
            style_outputs = outputs[:self.num_style_layers]
            content_outputs = outputs[self.num_style_layers:]

            # üé® Calcul des matrices de Gram pour le style
            style_features = []
            for i in range(self.num_style_layers):
                gram = gram_matrix(style_outputs[i])
                style_features.append(gram)

            # üì¶ Construction des dictionnaires de sortie
            content_dict = {}
            for i, content_name in enumerate(self.content_layers):
                content_dict[content_name] = content_outputs[i]

            style_dict = {}
            for i, style_name in enumerate(self.style_layers):
                style_dict[style_name] = style_features[i]

            return {'content': content_dict, 'style': style_dict}

    def perform_style_transfer(content_path, style_path, style_weight, content_weight, iterations):
        """
        üé® Fonction principale de transfert de style
        
        Analogie : Comme un peintre qui m√©lange deux techniques :
        - Il garde la forme et structure de son mod√®le (contenu)
        - Il applique la technique d'un ma√Ætre (style)
        
        Le processus est it√©ratif, comme un artiste qui am√©liore 
        progressivement son ≈ìuvre coup de pinceau par coup de pinceau.
        
        Args:
            content_path: Image de contenu (ce qu'on veut styliser)
            style_path: Image de style (l'art qu'on veut imiter)
            style_weight: Importance du style artistique
            content_weight: Importance de rester fid√®le au contenu
            iterations: Nombre d'am√©liorations √† faire
            
        Returns:
            Image stylis√©e finale
        """
        # üìã Configuration des couches d'analyse
        content_layers = ['block5_conv2']  # Couche s√©mantique profonde
        style_layers = build_style_layers_list()  # Selon s√©lection utilisateur
        
        if not style_layers:
            st.error("‚ùå Aucune couche de style s√©lectionn√©e ! Activez au moins une couche.")
            return None

        st.info(f"üé® Utilisation de {len(style_layers)} couches de style : {', '.join(style_layers)}")

        # üèóÔ∏è Cr√©ation du mod√®le extracteur
        extractor = StyleContentModel(style_layers, content_layers)

        # üõ†Ô∏è Pr√©processing des images d'entr√©e
        content_image = preprocess_image(content_path, max_dim=max_image_size)
        style_image = preprocess_image(style_path, max_dim=max_image_size)

        # üîç Affichage debug des images pr√©process√©es
        if debug_mode:
            st.write("üîç **V√©rification des images pr√©process√©es :**")
            col_debug1, col_debug2 = st.columns(2)
            
            with col_debug1:
                debug_content = deprocess_image(content_image.numpy())
                st.image(debug_content, caption=f"Contenu ({debug_content.shape[1]}√ó{debug_content.shape[0]})", width=200)
                
            with col_debug2:
                debug_style = deprocess_image(style_image.numpy())
                st.image(debug_style, caption=f"Style ({debug_style.shape[1]}√ó{debug_style.shape[0]})", width=200)

        # üéØ Extraction des cibles (ce qu'on veut atteindre)
        # Analogie : Prendre des "mesures" de l'≈ìuvre de r√©f√©rence
        style_targets = extractor(style_image)['style']
        content_targets = extractor(content_image)['content']

        # üé® Initialisation de l'image de travail
        # On commence avec l'image de contenu et on la modifie progressivement
        image = tf.Variable(content_image, dtype=tf.float32)
        
        # ‚öôÔ∏è Configuration de l'optimiseur Adam
        # Analogie : R√©gler les param√®tres du "pinceau intelligent"
        opt = tf.optimizers.Adam(
            learning_rate=learning_rate,
            beta_1=beta1,  # M√©moire des gradients
            beta_2=beta2,  # M√©moire de la variance
            epsilon=epsilon  # Stabilit√© num√©rique
        )

        # üìä Interface de progression
        progress_bar = st.progress(0)
        status_text = st.empty()
        loss_placeholder = st.empty()
        preview_placeholder = st.empty()
        
        def train_step(image):
            """
            üéØ Une √©tape d'am√©lioration
            
            Analogie : Un coup de pinceau guid√© par l'intelligence artificielle.
            L'IA regarde l'image actuelle, calcule ce qui ne va pas,
            et applique une petite correction.
            """
            with tf.GradientTape() as tape:
                # üîç Analyse de l'image actuelle
                outputs = extractor(image)
                
                # ‚öñÔ∏è Calcul de l'erreur (perte)
                loss = style_content_loss(
                    outputs, style_targets, content_targets, 
                    style_weight, content_weight
                )

            # üìê Calcul des gradients (direction d'am√©lioration)
            grad = tape.gradient(loss, image)
            
            # üé® Application de la correction
            opt.apply_gradients([(grad, image)])
            
            # üõ°Ô∏è Maintien des valeurs dans [0,1]
            image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0))
            
            return loss

        # üé® Boucle principale d'am√©lioration artistique
        st.write(f"üé® D√©but du transfert de style avec {iterations} it√©rations...")
        
        best_loss = float('inf')
        best_image = None
        
        for i in range(iterations):
            try:
                # üéØ Une √©tape d'am√©lioration
                loss = train_step(image)
                loss_value = float(loss)
                
                # üìà Suivi du meilleur r√©sultat
                if loss_value < best_loss:
                    best_loss = loss_value
                    best_image = image.numpy().copy()
                
                # üìä Mise √† jour de l'interface
                progress = (i + 1) / iterations
                progress_bar.progress(progress)
                
                # üìà Affichage d√©taill√© du statut
                status_text.markdown(f"""
                **It√©ration {i+1}/{iterations}**
                - Perte actuelle: {loss_value:.4f}
                - Meilleure perte: {best_loss:.4f}
                - Progression: {progress:.1%}
                """)
                
                # üîç Aper√ßu p√©riodique en mode debug
                if debug_mode and (i + 1) % preview_frequency == 0:
                    preview_img = deprocess_image(image.numpy())
                    with preview_placeholder.container():
                        col1, col2 = st.columns(2)
                        with col1:
                            st.image(preview_img, caption=f"Aper√ßu - It√©ration {i+1}", width=300)
                        with col2:
                            st.metric("Perte", f"{loss_value:.4f}", f"{loss_value - best_loss:.4f}")
                
            except Exception as e:
                st.error(f"‚ùå Erreur √† l'it√©ration {i+1}: {str(e)}")
                break

        # üßπ Nettoyage de l'interface
        progress_bar.empty()
        status_text.empty()
        preview_placeholder.empty()
        
        # üèÜ Retour du meilleur r√©sultat trouv√©
        return tf.constant(best_image) if best_image is not None else image

    # üñºÔ∏è Interface utilisateur pour le transfert de style
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### üì∏ Image de Contenu")
        st.write("*L'image que vous voulez transformer*")
        content_file = st.file_uploader(
            "Choisissez l'image de contenu", 
            type=["jpg", "png", "jpeg"], 
            key="content",
            help="Votre photo personnelle qui sera stylis√©e"
        )
        
        if content_file:
            st.image(content_file, caption="Image de contenu", use_column_width=True)
            if show_diagnostics:
                diagnose_image(content_file, "contenu")
    
    with col2:
        st.markdown("### üé® Image de Style")
        st.write("*L'≈ìuvre d'art dont vous voulez copier le style*")
        style_file = st.file_uploader(
            "Choisissez l'image de style", 
            type=["jpg", "png", "jpeg"], 
            key="style",
            help="Une peinture, dessin ou ≈ìuvre d'art dont vous aimez le style"
        )
        
        if style_file:
            st.image(style_file, caption="Image de style", use_column_width=True)
            if show_diagnostics:
                diagnose_image(style_file, "style")
    
    # üé® Configuration rapide pr√©d√©finie
    st.markdown("### ‚ö° Configurations Rapides")
    col_presets1, col_presets2, col_presets3 = st.columns(3)
    
    with col_presets1:
        if st.button("üñºÔ∏è Portrait Artistique"):
            # Configuration optimale pour portraits
            st.session_state.update({
                'style_weight': 5e3,
                'content_weight': 1e4, 
                'learning_rate': 0.008,
                'iterations': 150
            })
            st.success("Configuration portrait appliqu√©e !")
    
    with col_presets2:
        if st.button("üèûÔ∏è Paysage Stylis√©"):
            # Configuration optimale pour paysages
            st.session_state.update({
                'style_weight': 8e3,
                'content_weight': 1e3,
                'learning_rate': 0.012, 
                'iterations': 100
            })
            st.success("Configuration paysage appliqu√©e !")
    
    with col_presets3:
        if st.button("‚ö° Test Rapide"):
            # Configuration pour test rapide
            st.session_state.update({
                'style_weight': 1e4,
                'content_weight': 1e3,
                'learning_rate': 0.02,
                'iterations': 50
            })
            st.success("Configuration test appliqu√©e !")
    
    # üìä Pr√©diction de temps de calcul
    if content_file and style_file:
        # Estimation bas√©e sur la taille et les it√©rations
        estimated_time = (max_image_size / 512) ** 2 * iterations * 0.05
        st.info(f"‚è±Ô∏è Temps estim√© : {estimated_time:.1f} minutes")
        
        if estimated_time > 10:
            st.warning("‚ö†Ô∏è Temps long pr√©vu. Consid√©rez r√©duire la taille d'image ou les it√©rations.")

    # üöÄ Bouton principal de lancement
    if st.button("üé® Lancer le Transfert de Style", type="primary", use_container_width=True):
        if content_file and style_file:
            # üé¨ D√©but du processus
            start_time = st.empty()
            current_time = datetime.now().strftime('%H:%M:%S')
            start_time.write(f"üöÄ **D√©marrage du transfert de style √† {current_time}**")
            
            with st.spinner('üé® Transfert de style en cours... Votre IA cr√©e une ≈ìuvre d\'art !'):
                try:
                    process_start = time.time()
                    
                    # üé® Ex√©cution du transfert de style
                    stylized_image = perform_style_transfer(
                        content_file, style_file, 
                        style_weight, content_weight, iterations
                    )
                    
                    if stylized_image is not None:
                        # ‚è±Ô∏è Calcul du temps √©coul√©
                        process_time = time.time() - process_start
                        
                        # üñºÔ∏è Conversion et affichage du r√©sultat
                        result_image = deprocess_image(stylized_image.numpy())
                        
                        # üéØ Section des r√©sultats
                        st.markdown("## üéØ R√©sultat du Transfert de Style")
                        
                        # üìä Comparaison avant/apr√®s
                        col_before, col_after = st.columns(2)
                        
                        with col_before:
                            st.markdown("#### üì∏ Avant (Original)")
                            st.image(content_file, use_column_width=True)
                        
                        with col_after:
                            st.markdown("#### üé® Apr√®s (Stylis√©)")
                            st.image(result_image, use_column_width=True)
                        
                        # üìà Statistiques du processus
                        st.markdown("### üìä Statistiques du Processus")
                        col_stats1, col_stats2, col_stats3, col_stats4 = st.columns(4)
                        
                        with col_stats1:
                            st.metric("‚è±Ô∏è Temps", f"{process_time:.1f}s")
                        with col_stats2:
                            st.metric("üîÑ It√©rations", iterations)
                        with col_stats3:
                            st.metric("üìè Taille", f"{max_image_size}px")
                        with col_stats4:
                            efficiency = iterations / process_time if process_time > 0 else 0
                            st.metric("‚ö° Vitesse", f"{efficiency:.1f} it/s")
                        
                        # üé® Informations sur la configuration utilis√©e
                        with st.expander("üîß Configuration Utilis√©e"):
                            st.write(f"**Poids du style :** {style_weight:.0e}")
                            st.write(f"**Poids du contenu :** {content_weight:.0e}")
                            st.write(f"**Taux d'apprentissage :** {learning_rate}")
                            st.write(f"**Couches de style :** {', '.join(build_style_layers_list())}")
                            st.write(f"**Optimiseur Adam :** Œ≤‚ÇÅ={beta1}, Œ≤‚ÇÇ={beta2}, Œµ={epsilon:.0e}")
                        
                        # üíæ T√©l√©chargement du r√©sultat
                        result_pil = Image.fromarray(result_image)
                        
                        # üé® Options de sauvegarde
                        st.markdown("### üíæ T√©l√©chargement")
                        
                        col_download1, col_download2 = st.columns(2)
                        
                        with col_download1:
                            # PNG haute qualit√©
                            buf_png = io.BytesIO()
                            result_pil.save(buf_png, format='PNG', optimize=True)
                            
                            st.download_button(
                                label="üì• T√©l√©charger PNG (Haute Qualit√©)",
                                data=buf_png.getvalue(),
                                file_name=f"saffire_stylized_{int(time.time())}.png",
                                mime="image/png",
                                use_container_width=True
                            )
                        
                        with col_download2:
                            # JPEG optimis√©
                            buf_jpg = io.BytesIO()
                            result_pil.save(buf_jpg, format='JPEG', quality=95, optimize=True)
                            
                            st.download_button(
                                label="üì• T√©l√©charger JPEG (Optimis√©)",
                                data=buf_jpg.getvalue(),
                                file_name=f"saffire_stylized_{int(time.time())}.jpg",
                                mime="image/jpeg",
                                use_container_width=True
                            )
                        
                        # üéâ Message de succ√®s avec conseils
                        st.success("‚úÖ Transfert de style termin√© avec succ√®s !")
                        
                        # üí° Conseils pour am√©liorer
                        st.markdown("### üí° Conseils pour am√©liorer le r√©sultat")
                        st.info("""
                        **Pour plus de style artistique :** Augmentez le poids du style
                        
                        **Pour pr√©server plus l'original :** Augmentez le poids du contenu
                        
                        **Si l'image semble floue :** Augmentez le nombre d'it√©rations
                        
                        **Si les couleurs sont ternes :** Ajustez la saturation dans les param√®tres avanc√©s
                        """)
                        
                        # üìä Analyse de qualit√© automatique
                        st.markdown("### üîç Analyse de Qualit√©")
                        
                        # Calculs simples de qualit√©
                        original_array = np.array(Image.open(content_file).resize((256, 256)))
                        result_resized = np.array(result_pil.resize((256, 256)))
                        
                        # Mesure de similarit√© (simple MSE)
                        mse = np.mean((original_array.astype(float) - result_resized.astype(float)) ** 2)
                        similarity = max(0, 100 - mse / 100)  # Score approximatif
                        
                        col_quality1, col_quality2 = st.columns(2)
                        with col_quality1:
                            st.metric("üéØ Similarit√© contenu", f"{similarity:.1f}%")
                        with col_quality2:
                            color_variance = np.var(result_resized)
                            st.metric("üåà Richesse couleurs", f"{color_variance:.0f}")
                    
                    else:
                        st.error("‚ùå Erreur lors du transfert de style.")
                        
                except Exception as e:
                    st.error(f"‚ùå Erreur lors du transfert de style : {str(e)}")
                    
                    # üîß Suggestions de r√©solution
                    st.markdown("### üîß Suggestions de r√©solution :")
                    st.write("1. V√©rifiez que vos images sont au format JPG/PNG")
                    st.write("2. Essayez de r√©duire la taille maximale d'image") 
                    st.write("3. R√©duisez le nombre d'it√©rations pour un test")
                    st.write("4. V√©rifiez qu'au moins une couche de style est activ√©e")
                    
        else:
            st.warning("‚ö†Ô∏è Veuillez charger une image de contenu ET une image de style.")

# üîÑ MODULE DE TRANSFORMATION INVERSE
elif main_mode == "Transformation Inverse":
    st.markdown("## üîÑ Module de Transformation Inverse")
    st.write("*R√©cup√©rez le contenu original ou extrayez le style d'une image stylis√©e*")
    
    # üéõÔ∏è Hyperparam√®tres pour la transformation inverse
    st.sidebar.markdown("### üîÑ Param√®tres de Transformation Inverse")
    
    # Type de transformation inverse
    inverse_mode = st.sidebar.radio(
        "Type de transformation:",
        ["Extraction de Contenu", "Extraction de Style", "D√©stylisation Compl√®te"],
        help="Choisissez quel aspect r√©cup√©rer de l'image stylis√©e"
    )
    
    # Intensit√© de la transformation inverse
    inverse_strength = st.sidebar.slider(
        "Intensit√© de r√©cup√©ration:",
        0.1, 2.0, 1.0, step=0.1,
        help="Plus √©lev√© = r√©cup√©ration plus agressive"
    )
    
    # Nombre d'it√©rations pour l'optimisation inverse
    inverse_iterations = st.sidebar.number_input(
        "It√©rations d'optimisation:",
        min_value=50, max_value=500, value=200, step=25,
        help="Plus d'it√©rations = meilleure qualit√© mais plus lent"
    )
    
    # Param√®tres avanc√©s
    st.sidebar.markdown("### üîß Param√®tres Avanc√©s")
    
    inverse_learning_rate = st.sidebar.slider(
        "Taux d'apprentissage inverse:",
        0.001, 0.05, 0.01, step=0.001,
        help="Vitesse de r√©cup√©ration - plus lent mais plus stable"
    )
    
    content_preservation = st.sidebar.slider(
        "Pr√©servation structure:",
        0.0, 2.0, 1.0, step=0.1,
        help="Force de pr√©servation de la structure originale"
    )
    
    # R√©gularisation pour √©viter les artefacts
    regularization_weight = st.sidebar.slider(
        "R√©gularisation (anti-artefacts):",
        0.0, 0.1, 0.01, step=0.005,
        help="√âvite les pixels aberrants et lisse le r√©sultat"
    )
    
    # Type de perte pour l'optimisation inverse
    loss_type = st.sidebar.selectbox(
        "Type de perte d'optimisation:",
        ["MSE", "Perceptual", "Mixed"],
        help="MSE=simple | Perceptual=r√©aliste | Mixed=√©quilibr√©"
    )
    
    # Options de post-traitement
    st.sidebar.markdown("#### üé® Post-traitement")
    
    enhance_details = st.sidebar.checkbox(
        "Am√©lioration des d√©tails",
        value=True,
        help="Renforce les contours et textures r√©cup√©r√©s"
    )
    
    noise_reduction = st.sidebar.slider(
        "R√©duction du bruit:",
        0.0, 1.0, 0.3, step=0.1,
        help="Lisse les artefacts de reconstruction"
    )
    
    color_correction = st.sidebar.checkbox(
        "Correction colorim√©trique",
        value=True,
        help="Ajuste automatiquement les couleurs r√©cup√©r√©es"
    )

    def create_inverse_model(target_size=(512, 512)):
        """
        üîÑ Cr√©ation du mod√®le de transformation inverse
        
        Analogie : Comme un "d√©tective artistique" qui analyse une ≈ìuvre
        pour retrouver les √©l√©ments originaux cach√©s dessous.
        
        Le mod√®le utilise un autoencoder avec skip connections pour
        reconstruire le contenu ou style original.
        
        Args:
            target_size: Taille de l'image de sortie
            
        Returns:
            Mod√®le TensorFlow pour transformation inverse
        """
        from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, Concatenate, UpSampling2D
        from tensorflow.keras.layers import LeakyReLU, BatchNormalization
        
        # üì• Entr√©e : Image stylis√©e
        inputs = Input(shape=(*target_size, 3), name='stylized_input')
        
        # üîΩ Encodeur - "Analyse de l'image stylis√©e"
        # Analogie : D√©composer l'image en √©l√©ments compr√©hensibles
        
        # Block 1: Extraction des features de base
        e1 = Conv2D(64, 3, padding='same', name='encoder_1')(inputs)
        e1 = LeakyReLU(alpha=0.2)(e1)
        e1 = BatchNormalization()(e1)
        
        # Block 2: Features interm√©diaires
        e2 = Conv2D(128, 3, strides=2, padding='same', name='encoder_2')(e1)
        e2 = LeakyReLU(alpha=0.2)(e2)
        e2 = BatchNormalization()(e2)
        
        # Block 3: Features profondes
        e3 = Conv2D(256, 3, strides=2, padding='same', name='encoder_3')(e2)
        e3 = LeakyReLU(alpha=0.2)(e3)
        e3 = BatchNormalization()(e3)
        
        # Block 4: Repr√©sentation latente
        e4 = Conv2D(512, 3, strides=2, padding='same', name='encoder_4')(e3)
        e4 = LeakyReLU(alpha=0.2)(e4)
        e4 = BatchNormalization()(e4)
        
        # üîº D√©codeur - "Reconstruction du contenu original"
        # Analogie : Remonter du puzzle d√©compos√© vers l'image originale
        
        # Block 1: D√©but de reconstruction
        d1 = Conv2DTranspose(256, 3, strides=2, padding='same', name='decoder_1')(e4)
        d1 = LeakyReLU(alpha=0.2)(d1)
        d1 = BatchNormalization()(d1)
        d1 = Concatenate()([d1, e3])  # Skip connection pour pr√©server les d√©tails
        
        # Block 2: Reconstruction interm√©diaire
        d2 = Conv2DTranspose(128, 3, strides=2, padding='same', name='decoder_2')(d1)
        d2 = LeakyReLU(alpha=0.2)(d2)
        d2 = BatchNormalization()(d2)
        d2 = Concatenate()([d2, e2])  # Skip connection
        
        # Block 3: Reconstruction finale
        d3 = Conv2DTranspose(64, 3, strides=2, padding='same', name='decoder_3')(d2)
        d3 = LeakyReLU(alpha=0.2)(d3)
        d3 = BatchNormalization()(d3)
        d3 = Concatenate()([d3, e1])  # Skip connection
        
        # üéØ Sortie finale
        outputs = Conv2D(3, 3, activation='tanh', padding='same', name='output')(d3)
        
        # Cr√©ation du mod√®le
        model = tf.keras.Model(inputs=inputs, outputs=outputs, name='InverseTransformModel')
        
        return model

    def perceptual_loss(y_true, y_pred, vgg_model):
        """
        üëÅÔ∏è Calcul de la perte perceptuelle
        
        Analogie : Au lieu de comparer pixel par pixel (comme un robot),
        on compare ce que "voit" un expert (r√©seau VGG19 pr√©-entra√Æn√©).
        
        Args:
            y_true: Image cible
            y_pred: Image pr√©dite
            vgg_model: Mod√®le VGG19 pour extraction de features
            
        Returns:
            Perte perceptuelle bas√©e sur les features VGG19
        """
        # Pr√©processing pour VGG19
        y_true_vgg = preprocess_input(y_true * 255.0)
        y_pred_vgg = preprocess_input(y_pred * 255.0)
        
        # Extraction des features
        true_features = vgg_model(y_true_vgg)
        pred_features = vgg_model(y_pred_vgg)
        
        # Calcul de la diff√©rence perceptuelle
        loss = 0
        for true_feat, pred_feat in zip(true_features, pred_features):
            loss += tf.reduce_mean(tf.square(true_feat - pred_feat))
        
        return loss

    def total_variation_loss(image):
        """
        üåä Perte de variation totale pour r√©duction du bruit
        
        Analogie : Comme lisser une surface rugueuse pour la rendre plus naturelle.
        Cette fonction p√©nalise les variations brutales entre pixels voisins.
        
        Args:
            image: Image √† lisser
            
        Returns:
            Perte de variation totale
        """
        # Diff√©rences horizontales et verticales
        h_diff = image[:, 1:, :, :] - image[:, :-1, :, :]
        w_diff = image[:, :, 1:, :] - image[:, :, :-1, :]
        
        # Somme des variations
        return tf.reduce_mean(tf.square(h_diff)) + tf.reduce_mean(tf.square(w_diff))

    def perform_inverse_transform(stylized_image, reference_image=None):
        """
        üîÑ Ex√©cution de la transformation inverse
        
        Analogie : Comme un restaurateur d'art qui enl√®ve les couches
        de peinture ajout√©es pour retrouver l'≈ìuvre originale en dessous.
        
        Args:
            stylized_image: Image stylis√©e √† transformer
            reference_image: Image de r√©f√©rence (optionnelle)
            
        Returns:
            Image avec transformation inverse appliqu√©e
        """
        # üèóÔ∏è Pr√©paration du mod√®le VGG19 pour perte perceptuelle
        vgg = VGG19(include_top=False, weights='imagenet')
        vgg.trainable = False
        
        # S√©lection des couches pour perte perceptuelle
        feature_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3']
        feature_outputs = [vgg.get_layer(name).output for name in feature_layers]
        feature_model = tf.keras.Model([vgg.input], feature_outputs)
        
        # üéØ Initialisation de l'image de travail
        # On commence avec l'image stylis√©e et on la modifie progressivement
        target_image = tf.Variable(stylized_image, dtype=tf.float32)
        
        # ‚öôÔ∏è Optimiseur pour la transformation inverse
        optimizer = tf.optimizers.Adam(learning_rate=inverse_learning_rate)
        
        # üìä Interface de progression
        progress_bar = st.progress(0)
        status_text = st.empty()
        preview_placeholder = st.empty()
        
        @tf.function
        def inverse_step():
            """
            üîÑ Une √©tape d'optimisation inverse
            
            Calcule et applique une correction pour se rapprocher
            de l'objectif de transformation inverse.
            """
            with tf.GradientTape() as tape:
                # üìä Calcul des diff√©rentes pertes
                total_loss = 0
                
                if loss_type in ["MSE", "Mixed"]:
                    # üìè Perte MSE simple (pixel par pixel)
                    if reference_image is not None:
                        mse_loss = tf.reduce_mean(tf.square(target_image - reference_image))
                        total_loss += mse_loss * inverse_strength
                
                if loss_type in ["Perceptual", "Mixed"]:
                    # üëÅÔ∏è Perte perceptuelle (bas√©e sur la vision)
                    if reference_image is not None:
                        perc_loss = perceptual_loss(reference_image, target_image, feature_model)
                        total_loss += perc_loss * inverse_strength * 0.1
                
                # üõ°Ô∏è R√©gularisation pour √©viter les artefacts
                if regularization_weight > 0:
                    tv_loss = total_variation_loss(target_image)
                    total_loss += tv_loss * regularization_weight
                
                # üèóÔ∏è Pr√©servation de la structure si demand√©e
                if content_preservation > 0:
                    structure_loss = tf.reduce_mean(tf.square(
                        tf.image.sobel_edges(target_image) - 
                        tf.image.sobel_edges(stylized_image)
                    ))
                    total_loss += structure_loss * content_preservation
            
            # üìê Calcul et application des gradients
            gradients = tape.gradient(total_loss, target_image)
            optimizer.apply_gradients([(gradients, target_image)])
            
            # üõ°Ô∏è Maintien des valeurs dans [0,1]
            target_image.assign(tf.clip_by_value(target_image, 0.0, 1.0))
            
            return total_loss
        
        # üîÑ Boucle d'optimisation inverse
        st.write(f"üîÑ D√©but de la transformation inverse ({inverse_mode})...")
        
        best_loss = float('inf')
        best_image = None
        
        for i in range(inverse_iterations):
            try:
                # üéØ Une √©tape d'am√©lioration
                loss = inverse_step()
                loss_value = float(loss)
                
                # üìà Suivi du meilleur r√©sultat
                if loss_value < best_loss:
                    best_loss = loss_value
                    best_image = target_image.numpy().copy()
                
                # üìä Mise √† jour de l'interface
                progress = (i + 1) / inverse_iterations
                progress_bar.progress(progress)
                
                status_text.markdown(f"""
                **It√©ration {i+1}/{inverse_iterations}**
                - Perte: {loss_value:.6f}
                - Meilleure: {best_loss:.6f}
                - Mode: {inverse_mode}
                """)
                
                # üîç Aper√ßu p√©riodique
                if (i + 1) % 25 == 0:
                    preview_img = deprocess_image(target_image.numpy())
                    with preview_placeholder.container():
                        st.image(preview_img, caption=f"Progression - It√©ration {i+1}", width=300)
                
            except Exception as e:
                st.error(f"‚ùå Erreur √† l'it√©ration {i+1}: {str(e)}")
                break
        
        # üßπ Nettoyage interface
        progress_bar.empty()
        status_text.empty()
        preview_placeholder.empty()
        
        # üé® Post-traitement si demand√©
        final_image = tf.constant(best_image) if best_image is not None else target_image
        
        if enhance_details:
            # üîç Am√©lioration des d√©tails via filtre passe-haut
            kernel = tf.constant([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]], dtype=tf.float32)
            kernel = tf.reshape(kernel, [3, 3, 1, 1])
            kernel = tf.tile(kernel, [1, 1, 3, 1])  # Pour les 3 canaux RGB
            
            details = tf.nn.conv2d(final_image, kernel, strides=[1, 1, 1, 1], padding='SAME')
            final_image = final_image + details * 0.1  # Ajout subtil des d√©tails
        
        if noise_reduction > 0:
            # üåä R√©duction du bruit par filtrage gaussien
            final_image = tf.image.gaussian_filter2d(final_image, sigma=noise_reduction)
        
        # üõ°Ô∏è Clipping final
        final_image = tf.clip_by_value(final_image, 0.0, 1.0)
        
        return final_image

    # üñºÔ∏è Interface utilisateur
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### üé® Image Stylis√©e")
        st.write("*L'image stylis√©e dont vous voulez extraire des √©l√©ments*")
        stylized_file = st.file_uploader(
            "Choisissez l'image stylis√©e", 
            type=["jpg", "png", "jpeg"], 
            key="stylized",
            help="L'image qui a subi un transfert de style"
        )
        
        if stylized_file:
            st.image(stylized_file, caption="Image stylis√©e", use_column_width=True)
    
    with col2:
        st.markdown("### üì∏ Image de R√©f√©rence (Optionnelle)")
        st.write("*L'image originale pour guider la transformation inverse*")
        reference_file = st.file_uploader(
            "Choisissez l'image de r√©f√©rence", 
            type=["jpg", "png", "jpeg"], 
            key="reference",
            help="L'image originale avant stylisation (optionnel)"
        )
        
        if reference_file:
            st.image(reference_file, caption="Image de r√©f√©rence", use_column_width=True)
    
    # ‚ÑπÔ∏è Explication du mode s√©lectionn√©
    if inverse_mode == "Extraction de Contenu":
        st.info("üéØ **Mode Extraction de Contenu** : R√©cup√®re les formes et structures originales en supprimant les effets de style")
    elif inverse_mode == "Extraction de Style":
        st.info("üé® **Mode Extraction de Style** : Isole les √©l√©ments stylistiques (textures, coups de pinceau) pour les r√©utiliser")
    else:
        st.info("üîÑ **Mode D√©stylisation Compl√®te** : Tente de retrouver l'image originale compl√®te avant stylisation")
    
    # üöÄ Bouton de lancement
    if st.button("üîÑ Lancer la Transformation Inverse", type="primary", use_container_width=True):
        if stylized_file:
            with st.spinner(f'üîÑ Transformation inverse en cours ({inverse_mode})...'):
                try:
                    start_time = time.time()
                    
                    # üõ†Ô∏è Pr√©paration des images
                    stylized_image = preprocess_image(stylized_file, max_dim=512)
                    reference_image = None
                    
                    if reference_file:
                        reference_image = preprocess_image(reference_file, max_dim=512)
                        # Redimensionner pour correspondre √† l'image stylis√©e
                        ref_shape = tf.shape(reference_image)
                        sty_shape = tf.shape(stylized_image)
                        if ref_shape[1] != sty_shape[1] or ref_shape[2] != sty_shape[2]:
                            reference_image = tf.image.resize(reference_image, [sty_shape[1], sty_shape[2]])
                    
                    # üîÑ Ex√©cution de la transformation inverse
                    result_image = perform_inverse_transform(stylized_image, reference_image)
                    
                    # ‚è±Ô∏è Calcul du temps
                    process_time = time.time() - start_time
                    
                    # üñºÔ∏è Affichage des r√©sultats
                    result_array = deprocess_image(result_image.numpy())
                    
                    st.markdown("## üéØ R√©sultat de la Transformation Inverse")
                    
                    # üìä Comparaison avant/apr√®s
                    if reference_file:
                        col_original, col_stylized, col_recovered = st.columns(3)
                        
                        with col_original:
                            st.markdown("#### üì∏ Original")
                            st.image(reference_file, use_column_width=True)
                        
                        with col_stylized:
                            st.markdown("#### üé® Stylis√©")
                            st.image(stylized_file, use_column_width=True)
                        
                        with col_recovered:
                            st.markdown("#### üîÑ R√©cup√©r√©")
                            st.image(result_array, use_column_width=True)
                    else:
                        col_before, col_after = st.columns(2)
                        
                        with col_before:
                            st.markdown("#### üé® Avant (Stylis√©)")
                            st.image(stylized_file, use_column_width=True)
                        
                        with col_after:
                            st.markdown("#### üîÑ Apr√®s (Transform√©)")
                            st.image(result_array, use_column_width=True)
                    
                    # üìà Statistiques
                    st.markdown("### üìä Statistiques du Processus")
                    col_stats1, col_stats2, col_stats3 = st.columns(3)
                    
                    with col_stats1:
                        st.metric("‚è±Ô∏è Temps", f"{process_time:.1f}s")
                    with col_stats2:
                        st.metric("üîÑ It√©rations", inverse_iterations)
                    with col_stats3:
                        efficiency = inverse_iterations / process_time if process_time > 0 else 0
                        st.metric("‚ö° Vitesse", f"{efficiency:.1f} it/s")
                    
                    # üíæ T√©l√©chargement
                    result_pil = Image.fromarray(result_array)
                    buf = io.BytesIO()
                    result_pil.save(buf, format='PNG')
                    
                    st.download_button(
                        label="üì• T√©l√©charger le R√©sultat",
                        data=buf.getvalue(),
                        file_name=f"saffire_inverse_{inverse_mode.lower().replace(' ', '_')}_{int(time.time())}.png",
                        mime="image/png",
                        use_container_width=True
                    )
                    
                    # ‚úÖ Message de succ√®s
                    st.success(f"‚úÖ Transformation inverse ({inverse_mode}) termin√©e avec succ√®s !")
                    
                    # üí° Conseils d'am√©lioration
                    st.markdown("### üí° Conseils pour Am√©liorer")
                    if inverse_mode == "Extraction de Contenu":
                        st.info("üí° Si le contenu n'est pas assez r√©cup√©r√©, augmentez l'intensit√© de r√©cup√©ration ou utilisez une image de r√©f√©rence")
                    elif inverse_mode == "Extraction de Style":
                        st.info("üí° Pour isoler mieux le style, essayez de r√©duire la pr√©servation de structure")
                    else:
                        st.info("üí° Pour une meilleure d√©stylisation, fournissez l'image originale comme r√©f√©rence")
                
                except Exception as e:
                    st.error(f"‚ùå Erreur lors de la transformation inverse : {str(e)}")
                    st.write("üîß **Solutions possibles :**")
                    st.write("- V√©rifiez le format de votre image (JPG/PNG)")
                    st.write("- R√©duisez le nombre d'it√©rations pour un test")
                    st.write("- Essayez avec une image plus petite")
        else:
            st.warning("‚ö†Ô∏è Veuillez charger au minimum une image stylis√©e.")
    
    # üìö Section d'information
    with st.expander("‚ÑπÔ∏è Comment fonctionne la Transformation Inverse ?"):
        st.markdown("""
        ### üîÑ Principe de la Transformation Inverse
        
        La transformation inverse tente de "d√©faire" les effets du transfert de style pour r√©cup√©rer 
        les √©l√©ments originaux cach√©s dans l'image stylis√©e.
        
        ### üß† Processus Technique
        
        **1. Analyse de l'Image Stylis√©e** üîç
        - D√©composition en features via un r√©seau encodeur-d√©codeur
        - Identification des √©l√©ments de contenu vs style
        - S√©paration des composantes visuelles
        
        **2. Optimisation Inverse** ‚öôÔ∏è
        - Utilisation de gradients pour "remonter le temps"
        - Minimisation de la diff√©rence avec l'objectif
        - R√©gularisation pour √©viter les artefacts
        
        **3. Reconstruction** üèóÔ∏è
        - Assemblage des √©l√©ments r√©cup√©r√©s
        - Post-traitement pour am√©liorer la qualit√©
        - Lissage et correction des couleurs
        
        ### üéØ Modes de Transformation
        
        **Extraction de Contenu** üì∏
        - R√©cup√®re les formes et structures
        - Supprime les textures artistiques
        - Id√©al pour retrouver la g√©om√©trie originale
        
        **Extraction de Style** üé®
        - Isole les √©l√©ments stylistiques
        - Garde les textures et coups de pinceau
        - Utile pour cr√©er des templates de style
        
        **D√©stylisation Compl√®te** üîÑ
        - Tente de retrouver l'image originale
        - Combine r√©cup√©ration de contenu et suppression de style
        - Meilleur r√©sultat avec image de r√©f√©rence
        
        ### ‚öôÔ∏è Param√®tres Cl√©s
        
        **Intensit√© de R√©cup√©ration** üí™
        - Contr√¥le la force de la transformation inverse
        - Plus √©lev√© = r√©cup√©ration plus agressive
        - Risque : artefacts si trop √©lev√©
        
        **Pr√©servation Structure** üèóÔ∏è
        - Maintient la g√©om√©trie de base
        - Important pour l'extraction de contenu
        - √âvite les d√©formations excessives
        
        **R√©gularisation** üõ°Ô∏è
        - √âvite les pixels aberrants
        - Lisse le r√©sultat final
        - √âquilibrer avec la qualit√© des d√©tails
        
        ### üí° Conseils d'Utilisation
        
        **Pour de Meilleurs R√©sultats** ‚ú®
        - Utilisez l'image originale comme r√©f√©rence si disponible
        - Commencez avec des param√®tres conservateurs
        - Augmentez progressivement l'intensit√©
        - Testez diff√©rents modes selon votre objectif
        
        **Limitations** ‚ö†Ô∏è
        - La transformation inverse n'est jamais parfaite
        - Certaines informations sont d√©finitivement perdues
        - La qualit√© d√©pend du niveau de stylisation initial
        - Plus l'image √©tait stylis√©e, plus difficile la r√©cup√©ration
        """)

# üìö Section d'information d√©taill√©e
    with st.expander("‚ÑπÔ∏è Comment fonctionne le Transfert de Style Neural ?"):
        st.markdown("""
        ### üß† Principe de Base
        
        Le transfert de style neural utilise l'intelligence artificielle pour **s√©parer** et **recombiner** 
        deux aspects d'une image :
        
        1. **Le Contenu** üì∏ : La structure, les formes, les objets (QUOI est dans l'image)
        2. **Le Style** üé® : Les textures, couleurs, coups de pinceau (COMMENT c'est peint)
        
        ### üî¨ Le Processus Technique
        
        **√âtape 1 - Analyse** üîç
        - L'IA "regarde" votre photo avec les "yeux" d'un r√©seau VGG19 pr√©-entra√Æn√©
        - Elle identifie les formes et objets (contenu) dans les couches profondes
        - Elle analyse les textures et patterns (style) dans plusieurs couches
        
        **√âtape 2 - Extraction des "Signatures"** üìä
        - **Contenu** : Features maps de la couche block5_conv2 (compr√©hension s√©mantique)
        - **Style** : Matrices de Gram des couches block1 √† block5 (corr√©lations de textures)
        
        **√âtape 3 - Optimisation It√©rative** üéØ
        - L'IA commence avec votre photo originale
        - √Ä chaque it√©ration, elle la modifie l√©g√®rement pour :
          - Garder le m√™me contenu (fid√©lit√© √† l'original)
          - Adopter le style de l'≈ìuvre d'art (transformation artistique)
        - Le processus s'arr√™te quand l'√©quilibre optimal est trouv√©
        
        ### ‚öñÔ∏è Les Hyperparam√®tres Expliqu√©s
        
        **Poids du Style vs Contenu** üéöÔ∏è
        - **Style √©lev√©** ‚Üí Plus artistique, moins ressemblant
        - **Contenu √©lev√©** ‚Üí Plus fid√®le, moins stylis√©
        - **√âquilibre** ‚Üí Transformation harmonieuse
        
        **Nombre d'It√©rations** üîÑ
        - Comme un peintre qui affine son ≈ìuvre
        - Plus d'it√©rations = meilleur r√©sultat mais plus lent
        - 50-100 pour test, 200-500 pour qualit√© finale
        
        **Taux d'Apprentissage** ‚ö°
        - Vitesse des "coups de pinceau" de l'IA
        - Trop rapide ‚Üí instable, trop lent ‚Üí convergence lente
        - 0.01 est g√©n√©ralement optimal
        
        ### üé® Conseils d'Utilisation
        
        **Choix des Images** üì∏
        - **Contenu** : Photos nettes, bien contrast√©es
        - **Style** : ≈íuvres d'art avec textures riches (Van Gogh, Picasso, etc.)
        
        **Premiers Tests** ‚ö°
        - Commencez avec la configuration "Test Rapide"
        - Ajustez selon le r√©sultat obtenu
        - Exp√©rimentez avec diff√©rents styles
        
        **Optimisation** üéØ
        - Portrait ‚Üí Privil√©gier le contenu
        - Paysage ‚Üí √âquilibrer style/contenu  
        - Art abstrait ‚Üí Privil√©gier le style
        """)
    
    # üé≠ Galerie d'exemples (si vous voulez ajouter des exemples)
    with st.expander("üñºÔ∏è Galerie d'Exemples et Inspirations"):
        st.markdown("""
        ### üé® Styles Artistiques Populaires
        
        **Impressionnisme** üåÖ
        - Van Gogh, Monet, Renoir
        - Effet : Coups de pinceau visibles, couleurs vives
        - Id√©al pour : Paysages, portraits
        
        **Cubisme** üî∑
        - Picasso, Braque
        - Effet : Formes g√©om√©triques, perspectives multiples
        - Id√©al pour : Portraits, objets
        
        **Art Japonais** üóæ
        - Hokusai, style manga
        - Effet : Lignes nettes, couleurs plates
        - Id√©al pour : Tous types d'images
        
        **Art Moderne** üé≠
        - Kandinsky, Mondrian
        - Effet : Abstraction, couleurs pures
        - Id√©al pour : Cr√©ations artistiques audacieuses
        
        ### üí° Astuces de Pro
        
        1. **Testez diff√©rents ratios style/contenu** pour le m√™me couple d'images
        2. **Utilisez des styles contrast√©s** avec votre photo pour des effets saisissants
        3. **Les ≈ìuvres avec textures prononc√©es** donnent de meilleurs r√©sultats
        4. **Combinez plusieurs passes** : style l√©ger puis style prononc√©
        5. **Post-traitez** : ajustez luminosit√©/contraste apr√®s le transfert
        """)
    
    # üîß Section de d√©pannage
    with st.expander("üõ†Ô∏è D√©pannage et R√©solution de Probl√®mes"):
        st.markdown("""
        ### ‚ùå Probl√®mes Courants
        
        **"L'image reste floue ou d√©form√©e"** üå´Ô∏è
        - **Cause** : Trop d'it√©rations ou learning rate trop √©lev√©
        - **Solution** : R√©duire les it√©rations √† 50-100, learning rate √† 0.005
        
        **"Le style ne s'applique pas assez"** üé®
        - **Cause** : Poids du style trop faible
        - **Solution** : Augmenter le poids du style √† 1e5 ou plus
        
        **"L'image originale dispara√Æt compl√®tement"** üì∏
        - **Cause** : Poids du contenu trop faible
        - **Solution** : Augmenter le poids du contenu √† 1e4 ou plus
        
        **"Le processus est tr√®s lent"** ‚è≥
        - **Cause** : Image trop grande ou trop d'it√©rations
        - **Solution** : R√©duire taille √† 256px, limiter √† 50-100 it√©rations
        
        **"Erreur de m√©moire"** üíæ
        - **Cause** : Image trop grande pour votre syst√®me
        - **Solution** : Utiliser 256px maximum, red√©marrer l'application
        
        **"Couleurs √©tranges ou satur√©es"** üåà
        - **Cause** : Probl√®me de normalisation ou contraste
        - **Solution** : Ajuster saturation et contraste dans param√®tres avanc√©s
        
        ### üîç Diagnostic Auto
        
        Activez le **"Diagnostic d√©taill√©"** pour voir :
        - Format et qualit√© de vos images
        - Probl√®mes potentiels d√©tect√©s
        - Suggestions d'optimisation automatiques
        
        ### üöÄ Optimisation Performance
        
        **Pour des r√©sultats plus rapides :**
        - Taille 256px, 50 it√©rations
        - D√©sactiver le mode d√©bogage
        - Utiliser moins de couches de style
        
        **Pour la meilleure qualit√© :**
        - Taille 512px minimum
        - 200-500 it√©rations
        - Toutes les couches de style activ√©es
        - Learning rate r√©duit (0.005)
        """)
    
    # üìä Panneau de monitoring avanc√© (si debug activ√©)
    if debug_mode and content_file and style_file:
        st.markdown("### üî¨ Monitoring Avanc√© (Mode Debug)")
        
        # Informations syst√®me
        col_sys1, col_sys2, col_sys3 = st.columns(3)
        
        with col_sys1:
            st.metric("üñ•Ô∏è Backend", "TensorFlow")
        with col_sys2:
            st.metric("üß† Mod√®le", "VGG19 ImageNet")
        with col_sys3:
            st.metric("‚ö° Device", "CPU" if not tf.config.list_physical_devices('GPU') else "GPU")

# üìù Footer avec informations
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: gray; padding: 20px;'>
    <p><strong>SAFFIRE Detection System</strong> - Powered by TensorFlow & Streamlit</p>
    <p>üé® Module Classification: D√©tection intelligente de feu et fum√©e</p>
    <p>üñºÔ∏è Module Style Transfer: Transformation artistique par IA</p>
    <p><em>D√©velopp√© pour allier s√©curit√© et cr√©ativit√©</em></p>
</div>
""", unsafe_allow_html=True)